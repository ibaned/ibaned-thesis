%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER SIX                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{CONCLUSIONS AND FUTURE WORK}

\section{Conclusions}

We presented two implementations of general, conformal,
cavity-based mesh adaptation with a focus on high-performance
execution on heterogeneous supercomputers.
Two techniques for supporting mesh adaptation using array-based
data structures are presented, one of which uses rapidly modifiable
arrays and the second uses batched independent sets of modifications
to static arrays.
Highly scalable techniques for the parallelization of mesh adaptation are
developed, including the implementation of irregular sparse
communications using MPI, as well as the efficient interaction of
MPI with shared-memory array parallelism.
One of the implementations of mesh adaptation presented is capable of carrying out
general mesh adaptation in the context of the restricted programming
model required to execute on GPUs, and effectively takes advantage
of GPU parallelism.
Some exploration of mesh adaptation operators and their scheduling
is carried out, and the trade-offs of different approaches are
demonstrated.
Finally, the use of both implementations of mesh adaptation was demonstrated
by integrating them into the workflows of
several scientific simulation codes in use by the U.S. Army Corps of Engineers,
Sandia National Laboratories, Boeing, and others.

\section{Future Work}

\subsection{Convergence of PUMI and Omega\_h}
\label{sec:converge}

Section \ref{sec:omega_h-struct} presented the rationale
for the development of Omega\_h as a separate effort from PUMI,
which opens the question of whether it is possible
to combine the two codes in the future.
A combination in this case would be defined as a strict
union of their capabilities.
In order to retain the ability to execute efficiently
on GPUs and other shared memory hardware, a parallel
\texttt{for} loop programming model such as that introduced
in Section \ref{sec:openmp} would need to be used throughout.
The MDS structure design which allows single additions
and removals (see Section \ref{sec:sisc_soa}) would likely
need to be replaced with a static design and independent
set algorithms as is done in Omega\_h.
The resulting structure would need to be augmented to
accept more than just simplices, as described in Section \ref{sec:sisc_mstruct}.
Finally, several MeshAdapt algorithms not discussed
in this thesis would need to be re-designed to use parallel
\texttt{for} loops.
