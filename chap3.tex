%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER THREE                        %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{CAVITY-BASED CONFORMAL MESH ADAPTATION}
\label{chap:adapt}

\section{In Context}

The mesh adaptation methods in this work are both
conformal, general, and cavity-based.
They are conformal in the sense that the boundaries of all
elements (Section \ref{sec:def_complex}) are composed
of the set of entities expected by that element's
topological template (Section \ref{sec:topo_template}).
In other words, we avoid the ``hanging node" scenarios
introduced by non-conformal mesh modification techniques.
Typically, non-conformal mesh modification also restricts
itself to subdividing input elements into more elements,
or undoing such subdivisions which were done before.
Figure \ref{fig:hex_amr} illustrates such a method,
and clearly shows the hanging nodes introduced.

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{hex_amr.png}
\caption{Non-conforming parent-child adaptive mesh refinement
\cite{kirk2006libmesh}}
\label{fig:hex_amr}
\end{center}
\end{figure}

Non-conforming meshes require additional support from
the PDE-solving code to deal with hanging nodes, and typically
no more than one level of refinement is allowed between adjacent
elements.
The more important limitation is due to non-conforming methods
typically being parent-child methods, which fundamentally limits
them to the topology of the coarse input mesh.
If this input mesh is more fine than necessary in some areas,
it cannot be coarsened.
If moving objects or object deformation cause input elements
to become highly compressed or even inverted, parent-child
refinement can never correct or prevent this.
For these reasons we take a general approach, employing
coarsening, swapping, and possibly other operations which are
able to coarsen beyond the input mesh and correct low-quality
elements in the input mesh.

We restrict ourselves to local cavity operations as well,
meaning that the transformation from input to output meshes
can be expressed as a series of cavity modifications, each
of which can in turn be expressed as a the removal of
a small number of mesh entities followed by the addition
of a small number of mesh entities.
In this case small means a number which can be bounded
by a constant and is not affected by the total number
of mesh entities.

The benefits of using local cavity operations are:
\begin{enumerate}
\item It allows more straightforward and reliable parallelization of
mesh adaptation (see Section \ref{sec:cavity_sched}).
\item It allows much more careful control of the effects that mesh
adaptation has on the simulation fields attached to the mesh.
\end{enumerate}

On the other hand, the set of known cavity operations
have been found by the trial and error of researchers,
and there are many properties which they are not guaranteed to
achieve.
The most successful set of cavity operators are those
which operate on simplex meshes, due ultimately to the
fact that a simplex is the simplest polytope of a given dimension,
and that more complex polytopes require exponentially more
complex considerations.
In our work, we separate cavity operators into three categories:
\begin{enumerate}
\item Refinement: create a strictly more detailed discretization
than the input. Guaranteed not to invert elements, but not to
preserve any element quality. Guaranteed to exactly preserve
the distribution of fields.
\item Coarsening: create a strictly less detailed discretization
than the input. No guarantees it can be done without reducing
or negating quality, so it must be checked.
By definition, cannot exactly preserve the distribution of the fields.
\item Shape correction: typically maintains similar level of detail,
modifies connectivity to improve minimum element quality.
There is no known method guaranteed to raise all elements to a quality
that is useful for simulation and adaptation, but heuristic
methods can achieve great results in practice \cite{klingner2008aggressive}.
\end{enumerate}

\section{Related Work}

There have been several iterations of the MeshAdapt
library developed at RPI.
One of the earliest publications by De Cougny and Shephard
\cite{de1999parallel} outlines the three basic steps and goes into some detail
on a use of independent sets for coarsening purposes
(an idea that we extend significantly in Section \ref{sec:indset}.
Later, much work was done by Li on anisotropy using the metric
tensor and the selection of operators for shape correction
\cite{li20053d,li2003mesh}.
Our implementations of tetrahedral edge swaps make use
of guidance on fast implementation by Olivier-Gooch \cite{freitag1997tetrahedral}.
Researchers at INRIA have provided useful mathematical foundations
for handling the anisotropic metric tensor field
\cite{frey2005,alauzet2006parallel,loseille2015parallel},
and work at the Catholic University of Louvain explored
the use of mesh adaptation to respond to moving objects
\cite{compere2010mesh}, a path we continue with our Omega\_h work.

\section{MeshAdapt Methods}
\label{sec:ma_methods}

\subsection{Template Refinement}
\label{sec:ma_refine}

The MeshAdapt library uses edge-based refinement templates for its refinement step.
The way these work is that all edges whose metric length exceeds some threshold
$l_{\text{up}} > 1$ are marked for refinement.
Then each element takes into account the subset of its edges which are marked
for refinement, and chooses one of many possible subdivision patterns
(refinement templates) based on this subset of edges.
Figure \ref{fig:tet_templates} illustrates these templates in the case of tetrahedra.
In fact, the center template shown for three marked edges has two variants which
are symmetric by reflection but not by rotation.
In total, this means there are 12 rotationally unique tetrahedron refinement templates.

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{tet_templates.png}
\caption{Tetrahedral refinement templates}
\label{fig:tet_templates}
\end{center}
\end{figure}

One benefit of the use of refinement templates is that adjacent elements can be refined
simultaneously, so all edges, faces, and regions of the mesh can be modified in
a nearly embarrassingly parallel fashion once the set of marked edges is identified.
Another benefit is that the gradation of the mesh is more explicitly controlled
compared to methods which split edges independently.
However, refinement templates have some drawbacks as well:
\begin{enumerate}
\item In some cases, a subset of the template is a polyhedron that cannot be
subdivided into tetrahedra without introducing an extra vertex within the
parent tetrahedron.
In particular, Sch{\"o}nhardt's polyhedron can appear (see Figure
\ref{fig:schonhardt}).
This reduces the predictability of refinement and makes it more difficult
to transfer solution.
\item Other cases introduce a geometric decision, such as the case
when all edges of a tetrahedron are refined, or even when two edges
of a triangle are refined. This also reduces predictability.
\item It takes substantial code to implement all rotationally unique
combinations for all the relevant element polytopes.
This increases the likelihood of errors and decreases the productivity
of modifying any aspect of refinement.
\item Due to the simultaneous nature of the operation and the difficulty
of predicting the outcome, it is prohibitively difficult to reject
a local portion of the refinement based on criteria such as new elements
having too low quality.
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[width=0.2\textwidth]{schonhardt.png}
\caption{Sch{\"o}nhardt's irreducible polyhedron
\cite{Schonhardt1928}}
\label{fig:schonhardt}
\end{center}
\end{figure}

\subsection{Coarsening}
\label{sec:ma_coarsen}

Like other adaptation libraries, MeshAdapt implements
coarsening via edge collapses.
Figure \ref{fig:collapse} shows a typical edge collapse
in a tetrahedral mesh for reference.
One vertex in the mesh is ``moved" onto another vertex which is adjacent
via a mesh edge, collapsing this edge and all its adjacent
faces and regions.
For programming purposes, when dealing with sets of entities we
can call those being removed the ``collapsing" set and those being
conceptually elongated to fill the cavity as the set to ``keep".
In practice all old entities are removed and the set of entities to keep
is rebuilt with modified connectivity (where they were adjacent to the
collapsed vertex, now they are adjacent to the kept vertex, etc).

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{collapse.png}
\caption{Edge collapse in tetrahedral mesh
\cite{lu2011developments}}
\label{fig:collapse}
\end{center}
\end{figure}

Edge collapsing is an operation which, if not properly controlled,
can invalidate the mesh topologically or undo its topological similarity
to a CAD model.
MeshAdapt uses a set of checks which is somewhat expensive to compute
but is quite topologically robust:

\begin{enumerate}
\item The vertex being collapsed must have the same classification as
the edge being collapsed. This preserves similarity to the CAD model by
preventing collapses from the boundary into the interior.
\item If there exists a ring of three edges including the collapsing edge
(Figure \ref{fig:edge_ring}),
then those three edges must bound a single triangle in the mesh.
This prevents collapsing an empty hole in the mesh to zero volume as in
Figure \ref{fig:circle_hole}.
While some \cite{beall1997general} consider Figure \ref{fig:circle_hole}
a valid operation, current mesh data structures assume an entity is uniquely
defined by its set of bounding entities, which precludes the two overlapping
mesh edges that Figure \ref{fig:circle_hole} produces.
In addition, the non-collapsing edge adjacent to the collapsing vertex must have the same
classification as the triangle.
This prevents collapsing a cavity on a curved boundary down to zero volume
(see Figure \ref{fig:surf_collapse}), which would require re-classifying
entities on the cavity boundary (which breaks parallelism guarantees), and
creates no elements in the new cavity (which is problematic for solution transfer).
\item Analogous to the edge ring check, in 3D we check for two triangles which
share a non-collapsing edge and whose remaining two vertices are the endpoints
of the collapsing edge (a ``face ring").
The two triangles and the collapsing edge must bound a single tetrahedron,
and the triangle adjacent to the collapsing vertex must have the same classification
as this tetrahedron.
\item All the resulting tetrahedra must have positive volume.
In the 2D planar case, triangle normals should all be positive in the Z axis.
A relaxed version of this can be used to adapt triangulations of 2D surfaces
embedded in 3D, by requiring that the normals of new triangles be sufficiently
close to the normals of old triangles.
For straight-sided simplices in an equal-dimensional space (i.e. triangles in 2D),
this check on its own can prevent some topological invalidities, except
those in Figure \ref{fig:circle_hole} and Figure \ref{fig:surf_collapse}.
\end{enumerate}

The edge and face ring conditions are more complete versions of those described
by Garimella \cite{garimella1999anisotropic}.
Most of the expense of checking these conditions is in the search procedure
to identify edge rings and face rings, as the naive approach costs $n^2$
comparisons where $n$ is the number of edges (or faces) adjacent to one vertex.
For typical tetrahedral meshes there may be 36 or more faces adjacent to a vertex,
meaning close to 1000 comparisons would be needed.
We can reduce the cost by using a set structure to store the $n$ vertices (or edges)
that may complete a ring from one endpoint, and use its $\log(n)$ membership check
capability to reduce the comparison cost to $n\log(n)$.

\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{edge_ring.png}
\caption{Edge ring condition check during edge collapse}
\label{fig:edge_ring}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.4\textwidth]{circle_hole.png}
\caption{Illegal collapse of a CAD hole represented by a periodic boundary}
\label{fig:circle_hole}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.2\textwidth]{surf_collapse.png}
\caption{Illegal collapse with no new elements and re-classification}
\label{fig:surf_collapse}
\end{center}
\end{figure}

Finally, when applying a series of edge collapses to a mesh, one
must additionally filter the set of edges targeted for collapse
until it forms an independent set, in order to avoid a pathological
sequence of edge collapses reducing large portions of the mesh
down to a single edge.
We resolve this by visiting vertices which are marked for collapse
and unmarking them so long as each edge that needs collapsing
still keeps one adjacent marked vertex.
Similar methods of establishing an independent set were
in use in early versions of MeshAdapt \cite{de1999parallel}
and continue to be used by other adaptation codes
\cite{michal2012anisotropic}.

\subsection{Shape Correction}
\label{sec:ma_shape}

Shape correction, otherwise known in the literature as ``sliver removal",
is one of the most important open problems in tetrahedral meshing.
When meshing a planar domain with triangles, techniques such as
Delaunay refinement can guarantee triangles of a certain quality,
where quality can be measured by angles at corners or the
mean ratio (see Appendix \ref{app:vert_up_deg} for the relationship
between these measures).
However, when meshing a volumetric domain with tetrahedra, there
are no known methods to provably guarantee element quality (dihedral angles
or the mean ratio).
What is typically done is to carry out a method which satisfies
edge length criteria and at a minimum produces positive-volume
tetrahedra.
Following this, heuristic methods (sliver removal) are carried out
to attempt to remove tetrahedra with quality below a constant user-defined
threshold.

Despite these methods being heuristic, their combination can often
give very good results in practice.
The most aggressive approaches
\cite{klingner2008aggressive,dassi2016tetrahedral}
can usually bring tetrahedral dihedral angles into
the range $[30^\circ,130^\circ]$, which is sufficient for
most simulation purposes.
However, these aggressive approaches may be either too expensive
for use in mesh adaptation (which is executed within a
performance-critical simulation), or they may modify the mesh
too much (a large movement of all the nodes in the mesh would
change the physical distribution of the fields defined by values
at the nodes).

Mesh adaptation has the advantage of being provided with an input
mesh, and therefore one can in theory reject any operation which
would decrease the quality of the mesh lower than it was on input.
In this sense, mesh adaptation can guarantee quality at least as good
as the input.
We will see this approach taken to some degree in the Omega\_h methods
described in Section \ref{sec:omega_h-adapt}.
The danger of rejecting operations based on quality is that
if too many operations are rejected then edge lengths will
not conform well enough to the metric field.
However, recently researchers are having increasing success with
careful rejection of operations and are able to avoid using
sliver removal techniques \cite{loseille20093d,michal2012anisotropic}.
Attempting to reproduce their results is an area of immediate future work.

Typically, mesh adaptation libraries implement a balance
between prevention of quality degradation and repairing
quality that has been degraded.
The repair process uses a subset of the known sliver removal
techniques, with a focus on being able to repair the average
sliver using minimal computational resources, while resorting
to more expensive techniques when the cheaper ones fail.

Li describes a fairly comprehensive set of sliver removal
heuristics implemented in an earlier version of MeshAdapt
\cite{li2003mesh}.
The current version uses a similar approach, based on a
taxonomy of tetrahedra with mean ratio low quality value.
The taxonomy can be described in terms of some boundary
entity being too close to another boundary entity
of the tetrahedron:
\begin{enumerate}
\item In the first case, we have two vertices being too
close, which is really just an edge being too short.
If quality is low in metric space, this is a case
of an edge which should have collapsed but didn't.
We attempt to more aggressively collapse it by trying
to collapse each edge adjacent to either endpoint vertex.
\item In the second case, we have a vertex being too close
to its opposing triangle face.
Here we try to execute an edge swap (see Figure \ref{fig:swap})
on each of the three edges bounding the opposing triangle face.
(The work of Li suggests that a face split and collapse operation
should also be attempted as future work).
\item In the third case, we have the classic sliver tetrahedron
which has two opposing edges being too close to one another.
We first attempt to perform an edge swap on each of the two
opposing edges.
If that fails, we attempt a double split collapse operation
as shown in Figure \ref{fig:compound}.
\end{enumerate}
Each of the above attempted operations is judged in terms of quality.
If the minimum quality of any element in the cavity after a modification
exceeds minimum quality of any element in the cavity before the
modification, then we say the quality has locally improved.
In most cases, one of the above attempts succeeds.
However, it is possible in practice for none to succeed, in which
case MeshAdapt will leave the sliver as-is.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{split_collapse.png}
\caption{Double-split + collapse compound operator
\cite{li2003mesh}}
\label{fig:compound}
\end{center}
\end{figure}

\subsubsection{Edge Swap}
\label{sec:swap}

\begin{figure}
\begin{center}
\includegraphics[width=0.5\textwidth]{swap.png}
\caption{Edge swap in tetrahedral mesh
\cite{lu2011developments}}
\label{fig:swap}
\end{center}
\end{figure}

The edge swap operation deserves a detailed consideration due
to its complexity.
Its goal is to remove the elements adjacent to an edge and
replace them with a new set of elements such that the edge
is not recreated.
In the tetrahedral case, this essentially reduces to the problem
of forming an output triangular mesh based on the ``ring" vertices
(those vertices which are opposite to the central edge across
an input triangle), as Figure \ref{fig:swap} illustrates.
Furthermore, since this operation is used almost exclusively
for the purpose of repairing quality degradation (sliver removal),
what we are interested in is finding the triangular mesh of the
ring such that the resulting tetrahedral mesh of the cavity
contains elements above a certain quality.
The quality to beat is usually the lowest quality input
tetrahedron, i.e. we want to at least improve the minimum quality.

A naive search of the space of possible triangular meshes
can be quite expensive, so we use an optimized algorithm
described by Freitag and Ollivier-Gooch \cite{freitag1997tetrahedral}.
The algorithm consists of limiting oneself to rings of a
certain number of vertices (in our case, not attempting
rings that have more than 7 vertices), and storing integer
tables describing all possible trianglulations of such rings.
The triangulations are described in terms of unique triangles
(two triangulations may share the same triangle).
Because the qualities of the two adjacent tetrahedra adjacent
to such a triangle are uniquely defined, then if either quality
is below the target threshold, all triangulations that include
that triangle can be ignored.
Otherwise, the lower of the two qualities is stored in association
with the unique triangle, to avoid later re-computation.

\subsection{Overall Steps}

Listing \ref{lst:ma} shows an abbreviated version of the C++
code for MeshAdapt's main function.
The central loop executes a fixed number of iterations.
At each iteration, we first coarsen as described
in Section \ref{sec:ma_coarsen}, followed by refinement
as described in Section \ref{sec:ma_refine}.
We perform coarsening before refinement because this
should reduce the peak memory usage between the two
operations.
Shape correction is performed after the main loop
has completed, using the algorithm in Section \ref{sec:ma_shape}.
The functions whose names end in \texttt{Balance} execute
parallel load balancing.
We treat mesh elements as the units being balanced,
and assign weights to them based on the metric space
volume defined by Equation \ref{eq:metric_volume}.
Either graph partitioners \cite{devine2002zoltan} or
diffusive partitioning methods \cite{SmithParma2015}
may be used at each step.

\begin{lstlisting}[float,style=dan-style,caption=MeshAdapt main function,label=lst:ma]
void adapt(Input* in) {
  Adapt* a = new Adapt(in);
  preBalance(a);
  for (int i = 0; i < in->maximumIterations; ++i) {
    coarsen(a);
    midBalance(a);
    refine(a);
  }
  fixElementShapes(a);
  postBalance(a);
  delete a;
  delete in;
}
\end{lstlisting}

\section{Omega\_h Methods}
\label{sec:omega_h-adapt}

The Omega\_h code aims to provide MeshAdapt-like functionality
on a wide variety of computing hardware.
Due to the restrictions introduced by parallelism, Omega\_h
initially chose a simpler set of algorithms to reduce the
cost of redesigning each algorithm for portability.
Once the algorithms were redesigned, they actually demonstrated
an equivalent and in some cases better level of robustness,
meaning we can get more portable and performant results
with simpler operations.

\subsection{Refinement}
\label{sec:osh_refine}

Instead of using template-based refinement, Omega\_h
uses single edge splits, meaning a single edge is
bisected and adjacent elements are bisected
at a single operation.
Unlike the simultaneous execution that is possible
with template-based refinement, only edge
splits whose cavities do not overlap can be
applied simultaneously (see Section \ref{sec:indset}
for how such splits are selected).
Several researchers have similarly had success
using only single edge splits rather than
template-based refinement
\cite{compere2010mesh,loseille20093d,michal2012anisotropic}.
Like MeshAdapt, Omega\_h marks all edges whose
metric length exceeds some threshold (typically $4/3$) as candidates
for edge splits.
However, Omega\_h unmarks any edges whose splitting
would produce elements of low mean ratio quality (typically less than $20\%$).

\subsection{Coarsening}
\label{sec:osh_coarsen}

Like MeshAdapt, Omega\_h uses single edge collapses
for mesh coarsening.
Since all operations in Omega\_h use the quality-driven
independent set system described in Section \ref{sec:indset},
that explicitly takes care of the need for an independent
set of collapsing vertices.
Edge collapses have some confusion in what their ``key" entity is,
because in reality it is the combination of an edge and one
of its endpoint vertices that defines a collapse.
We first mark all edges whose metric length is beneath
some threshold (typically $2/3$) as candidates to collapse
via either endpoint.
Then, for all such edges, collapses via both endpoints are evaluated
using almost all of the checks described in Section \ref{sec:ma_coarsen}.
In particular, Omega\_h saves time by skipping the full edge ring and face
ring checks, replacing them with a requirement that each $d$-dimensional simplex
adjacent to the collapsing edge must be classified on the same geometry
as its $(d-1)$-dimensional side adjacent to the collapsing vertex.

In other words, Omega\_h checks for the problematic case shown in Figure
\ref{fig:surf_collapse}, but not for the case in Figure \ref{fig:circle_hole}.
As a result, Omega\_h requires that the input mesh, model or metric be
constructed such that the case in Figure \ref{fig:circle_hole} doesn't arise.
The simple way to ensure this is to mesh ``holes" in the geometry.
For Omega\_h's first target applications,
this makes sense because the interaction of both the fluid and the structure
is of interest.
Barring that solution, one can topologically break up periodic geometry
or use curvature-targeted metrics to ensure a periodic geometry is approximated
with more than three edges.

We mark candidate collapses using two boolean flags per edge, one for each
endpoint.
In addition to unmarking collapses that would violate the above topological
and classification checks, we also require the output elements to be
above some quality threshold.
Once all collapses that violate the checks mentioned above are unmarked,
we move the focus of the problem from edges to vertices.
To do this, every vertex chooses the highest-quality candidate edge collapse
for which it is an endpoint, and that collapse is the one it will
represent, all others for which it is an endpoint are discarded.
Now the independent set selection in Section \ref{sec:indset} is used
with vertices as the keys and their selected collapse's qualities as the
values.
The selected independent set of vertices is then collapsed, along each
vertex's chosen edge.

\subsection{Shape Correction}
\label{sec:osh_shape}

Omega\_h uses simpler and yet possibly more effective shape correction logic.
We have found that it is beneficial to consider a wider portion of the
mesh surrounding a sliver when attempting to remove said sliver, rather
than being overly concerned with the particular shape of said sliver.
Omega\_h has a user-defined parameter for how many layers of elements
around a sliver will be considered.
In this case, layers are defined by elements adjacent to one another
across faces, simply because this graph is much cheaper to compute
than that of elements adjacent via vertices.
For every sliver, elements that are at most the specified distance
via face adjacency are marked as being involved in sliver removal.
Then we do one of two things:
\begin{enumerate}
\item Mark all edges of all considered elements as candidates for edge swaps.
For each such edge we compute the best possible edge swap operation
using the optimizations described in Section \ref{sec:swap},
and give those quality values to the method in Section \ref{sec:indset} to
select a set of edge swaps to perform.
Only edge swaps which locally improve the quality in their cavity are accepted.
\item Mark all vertices of all considered elements as candidates for collapsing.
For each of these vertices, we consider collapsing it along any adjacent edge.
This is just a marking process, after which the procedure in Section \ref{sec:osh_coarsen}
is carried out, with the only difference being that for a collapse to be accepted,
the quality in the cavity must locally improve, instead of being above a
constant threshold.
\end{enumerate}
Note that although these algorithms describe using layers around a sliver,
they remain cheap because the layers need only be marked, after which only
small cavities around the marked elements are considered.
The full set of elements marked around a sliver is never collected or examined
as a whole in any way.
The marking itself can be done in a local iterative fashion, each step adding
marks to all face-adjacent elements of currently marked elements.

Unlike MeshAdapt, Omega\_h requires that all elements be above a certain
mean ratio quality, and will continue to attempt shape correction until
this goal is satisfied.
So far, we have been able to achieve minimum mean ratios between $20\%$
and $30\%$ for all mesh elements in a variety of cases.

\subsection{Overall Steps}
\label{sec:osh}

Listing \ref{lst:osh} shows an abbreviated version of the C++ code for
Omega\_h's main function.
The ordering of these operations was selected for the most part to minimize
the amount of work done, not to optimize memory usage or metric conformity.
First, refinement as described in Section \ref{sec:osh_refine} is done
until no more modifications are accepted.
Next, the same is done for coarsening as per \ref{sec:osh_coarsen}.
Finally, while the lowest element quality in the mesh is below some
threshold, we attempt to apply edge swaps and edge collapses as
per \ref{sec:osh_shape}.
Notice that edge swaps are strongly preferred, in fact edge
collapses as a quality repair solution are only attempted when
quality remains below the threshold and no legal edge swaps
would improve quality.

The parameter \texttt{qual\_floor} represents the quality that
we would prefer elements to always exceed during all parts of
adaptation.
There is an exception for the case when the input mesh is
already worse than that, hence the logic for computing
\texttt{allow\_qual}, which is the quality actually enforced
on modification operations.
The parameter \texttt{qual\_ceil} is the quality we would prefer
elements exceed \emph{after} mesh adaptation, and hence is the
target for shape correction operations.

\begin{lstlisting}[float,style=dan-style,caption=Omega\_h main function,label=lst:osh]
bool adapt(Mesh* mesh, Real qual_floor, Real qual_ceil, Real len_floor,
    Real len_ceil, Int nlayers) {
  auto input_qual = mesh->min_quality();
  auto allow_qual = min2(qual_floor, input_qual);
  while (refine_by_size(mesh, len_ceil, allow_qual));
  while (coarsen_by_size(mesh, len_floor, allow_qual));
  while (mesh->min_quality() < qual_ceil) {
    if (swap_edges(mesh, qual_ceil, nlayers)) continue;
    if (coarsen_slivers(mesh, qual_ceil, nlayers)) continue;
    break;
  }
  return true;
}
\end{lstlisting}

\section{Size Field Algorithms}
\label{sec:sf}

\subsection{Metric Interpolation and Storage}
\label{sec:metric_interp}

The metric tensor field introduced in Section \ref{sec:def_metric}
is typically provided as a set of values defined at mesh vertices.
This leads to an important question about how to compute metric
tensor values at other points in the mesh, i.e. how to interpolate
the metric tensor.
We can illustrate the issues involved with the simplest interpolation
case, that of deriving a metric tensor for the center of a mesh
edge given the two metric tensors at its endpoints.
Unfortunately, linear interpolation of the components of the
tensor $\mathcal{M}$ itself has the undesirable property that
interpolating between two highly anisotropic metrics of even
slightly different orientations will tend to produce isotropic
results with very small desired lengths, i.e. the longer desired
length will be lost.
For this reason, alternative interpolation methods have been developed
focusing on getting desired results in the case of high anisotropy.
Some of the most relevant are as follows:

\begin{enumerate}
\item MeshAdapt works with an internal representation consisting
of the orthogonal matrix $R$ and the vector of desired lengths
$\mathbf{h} = (h_1, h_2, h_3)^T$ which form the metric tensor:
\begin{equation}
\mathcal{M} = R^T \begin{bmatrix}
1 / h_1^2 & 0 & 0 \\
0 & 1 / h_2^2 & 0 \\
0 & 0 & 1 / h_3^2 \\
\end{bmatrix} R
\end{equation}
Each of these is then interpolated separately.
The vector $\mathbf{h}$ is linearly interpolated, while the
matrix $R$ is first linearly interpolated, and then re-orthogonalized
using the Gram-Schmidt process \cite{trefethen1997numerical}.
The benefit of this is that anisotropy is fully preserved due to the
separate interpolation of lengths.
However, there are three main drawbacks to this method:
\begin{enumerate}
\item The interpolation of $R$ is fragile, because if the linearly
interpolated value is rank-deficient then re-orthogonalization
will fail. This can happen, for example, if the two input $R$ matrices
represent rotations that are $180^{\circ}$ away from each other.
Even though these represent exactly the same metric tensor, interpolation
would fail.
\item The overall method gives results which depend too much on the
details of how the tensor was decomposed.
If we have two metrics which are $90^{\circ}$ away from each other
as shown in Figure \ref{fig:90deg_interp},
their interpolated result depends heavily on the signs of the eigenvectors
chosen for $R$.
\item This method stores 12 components per vertex on a 3D mesh, whereas
the alternatives below store only 6.
\end{enumerate}
\item Omega\_h computes the inverse of the metric tensors, then
interpolates those linearly, and inverts the result.
This is a method proposed by Alauzet and Frey as a compromise between
the anisotropic fidelity and high runtime costs of the following two methods
\cite{alauzet2003estimateur}.
\item The highest fidelity interpolation suggested by Alauzet and Frey is
the Power method which linearly interpolate $\mathcal{M}^{-1/2}$,
which equals $\mathcal{M}$ replacing each eigenvalue $\lambda$
with $(1/\sqrt{\lambda})$.
This requires the additional expense of computing an eigendecomposition.
\item An even better fidelity interpolation suggested by Loseille and
L{\"o}hner \cite{loseille20093d} and later confirmed by
Michal and Krakos \cite{michal2012anisotropic} is the Log-Euclidean
method which linearly interpolates $\log(\mathcal{M})$, which equals
$\mathcal{M}$ replacing the eigenvalues $\lambda$ with $\log(\lambda)$.
Figure \ref{fig:log_interp} shows how this method better preserves
very high levels of anisotropy.
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{90deg_interp.png}
\caption{MeshAdapt interpolation depends on eigenvector signs}
\label{fig:90deg_interp}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.95\textwidth]{log_interp.png}
\caption{Log-Euclidean versus Power interpolation
at 1:1000 anisotropy \cite{michal2012anisotropic}}
\label{fig:log_interp}
\end{center}
\end{figure}

To date the former two methods have proved sufficient for certain
degrees of anisotropy, yet we consider the implementation of one
of the latter two methods as an area of immediate future work.

\subsection{Identity Metric Field}
\label{sec:ident_metric}

For any simplex element, there exists a single metric tensor
such that all its edges are unit length in metric space, which
implies it is equilateral and has a perfect mean ratio quality
in metric space.
To find this metric, consider that each edge $i$ of the simplex
forms a single scalar constraint:

\begin{equation}
1 = \sqrt{\mathbf{v}_i^T \mathcal{M} \mathbf{v}_i}
\end{equation}

Where $\mathbf{v}_i$ is the real space vector along edge $i$.
Also note that the metric tensor $\mathcal{M}$ has exactly as
many independent scalars as the simplex has edges
($3$ in 2D, $6$ in 3D).
The constraint can be converted into a scalar equation
for the independent variables:

\begin{gather*}
1^2 = \left(\sqrt{\mathbf{v}_i^T \mathcal{M} \mathbf{v}_i}\right)^2 \\
1 = \mathbf{v}_i^T \mathcal{M} \mathbf{v}_i \\
1 = \begin{bmatrix} x_i & y_i & z_i \end{bmatrix}
\begin{bmatrix}
a & d & f \\
d & b & e \\
f & e & c \\
\end{bmatrix}
\begin{bmatrix}
x_i \\
y_i \\
z_i \\
\end{bmatrix} \\
1 = ax_i^2 + by_i^2 + cz_i^2 + 2dx_iy_i + 2ey_iz_i + 2fx_iz_i
\end{gather*}

This leads to a linear system, which following our 3D example
is a $6\times 6$ system:

\begin{equation}
\begin{bmatrix}
x_1^2 & y_1^2 & z_1^2 & 2x_1y_1 & 2y_1z_1 & 2x_1z_1 \\
x_2^2 & y_2^2 & z_2^2 & 2x_2y_2 & 2y_2z_2 & 2x_2z_2 \\
x_3^2 & y_3^2 & z_3^2 & 2x_3y_3 & 2y_3z_3 & 2x_3z_3 \\
x_4^2 & y_4^2 & z_4^2 & 2x_4y_4 & 2y_4z_4 & 2x_4z_4 \\
x_5^2 & y_5^2 & z_5^2 & 2x_5y_5 & 2y_5z_5 & 2x_5z_5 \\
x_6^2 & y_6^2 & z_6^2 & 2x_6y_6 & 2y_6z_6 & 2x_6z_6 \\
\end{bmatrix}
\begin{bmatrix}
a \\ b \\ c \\ d \\ e \\ f
\end{bmatrix}
\begin{bmatrix}
1 \\ 1 \\ 1 \\ 1 \\ 1 \\ 1
\end{bmatrix}
\end{equation}

Where $y_3$ is the $y$ component of the length of edge 3,
and so on.
The matrix and right hand side are formed based on the element
and the linear system is solved via QR decomposition
\cite{trefethen1997numerical}, giving us what we call
the ``identity" metric tensor for an element.
This derivation can be repeated for triangles in 2D, giving
a $3\time 3$ system for the symmetric tensor components.

In order to compute metric tensors at vertices,
We need to conceptually average the identity values obtained
at adjacent elements.
First, we transform the element metric tensors into their
``linear" form, which is one of the forms described
in Section \ref{sec:metric_interp} used for metric interpolation
(for example, Omega\_h uses the matrix inverse).
Then we take at each vertex the average of these linear
tensors at adjacent elements.
Applying the inverse ``de-linearizing" transformation to the resulting tensor
retries the metric tensor at the vertex.

\subsection{Identity Isotropic Size}

\subsection{Targeting an Element Count}

\section{Solution Transfer in a Cavity}

Emphasize numerical / performance advantage
over full-mesh methods.
Results can be included for each particular
method if it warrants one.

\subsection{Conserving Integral Quantities}

\section{Serial Adaptation Performance}

{\bf TODO... }

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:


