%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER ONE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{INTRODUCTION AND BACKGROUND}
\let\thefootnote\relax\footnotetext{
Portions of this chapter submitted as:
D. Ibanez and M. Shephard,
``Modifiable Array Data Structures for Mesh Topology,"
in \textit{SIAM J. Scientific Comput.},
under review.
}
\label{chap:intro}

\section{Introduction}
\label{sec:intro}

A wide variety of aerospace, mechanical, and nuclear engineering
problems require solving complex Partial Differential
Equations (PDEs) in time and space.
For efficiency and reliability, the solutions to these PDEs are
found using computers.

Computers are equipped with a limited amount of memory to
store information, and must use a mathematical representation
of a PDE solution that can be described using
a limited amount of information.
For many engineering problems of interest, the exact solution
as described by any known representation would require an
infinite amount of information, therefore approximate
solutions are sought.

A certain minimal amount of memory and processing power
are required to obtain the lowest accuracy approximate
solutions, and obtaining more accurate solutions requires
more memory and/or processing power.
For these reasons, computers with ever-increasing amounts
of memory and processing power are designed and built to
increase the accuracy of existing solutions and
to solve previously unsolvable engineering problems.
At any given point in history, the computers with the
most memory and processing power are called supercomputers.

Supercomputers are expensive to acquire and even more
expensive to operate in terms of electricity, cooling, and other
maintenance, so their ability to
produce accurate results at the lowest cost
is of critical importance.

When solving PDEs over general domains, it is common to
employ equation discretization methods that operate on a mesh,
which is more precisely defined in Section \ref{sec:def_mesh}.
Chapter \ref{chap:struct} presents the full design
and implementation of two new computer representations
of meshes, which are focused on minimizing the amount
of computer memory and processing required to use
an accurate mesh, and are also compatible with mesh adaptation.

Mesh adaptation is a way in which the spatial discretization
(mesh) can be altered over time to maximize the accuracy
of the solution obtainable with a certain amount
of computing power.
There are several open areas of research in mesh adaptation,
and Chapter \ref{chap:adapt} presents advancements
made in this work to design and implement
mesh adaptation that can run efficiently on present
and near-future supercomputers.

Present supercomputers make extensive use of distributed
memory parallelism by being constructed from tens of thousands of
smaller computers (nodes) connected by a fast network.
In an effort to reduce acquisition and maintenance costs
for a given amount of computing power,
present and future supercomputers will also make extensive use
of shared memory parallelism, by having each node be constructed
with hundreds to thousands of computing cores, all of which
are capable of working in parallel.
The combination of the two forms of parallelism is what
makes a supercomputer \emph{heterogeneous}.

Designing programs for heterogeneous supercomputers is a critical
challenge, because such programs must be able to precisely coordinate
a complex hierarchy of memory and interaction methods executing on
millions of compute cores
in order to solve a given problem at minimal cost.
Chapter \ref{chap:parallel} presents contributions to
the design and implementation of parallel programs,
including both widely applicable tools and tools specifically
designed to enable efficient parallel mesh adaptation.

An additional challenge to the design of parallel programs
is the fact that supercomputers fall into different
architectural design categories which currently
have very different programming interfaces.
Thus, in order to design a program which is \emph{portably performant}
over all architectures, one must try to abstract away
their differences and, unfortunately, cater to the
lowest common denominator of functionality.
Throughout this thesis, we present two systems:
the first only operates on some architectures and provides
a wide array of adaptive functionality, while the second
is portably performant across the major architectures
and currently provides less adaptive functionality.

Finally, Chapter \ref{chap:apps} presents several
application programs which make use of the tools developed
here in order to solve a variety of engineering problems.

\section{Terminology}

\begin{tabular}{l|l}
Topological Complex & A breakdown of a Cartesian domain \\
 & into topological entities \\
Mesh & A topological complex whose entities \\
 & have simple shape \\
Entity & A topological entity of a mesh \\
Vertex & A 0-dimensional entity \\
Edge & A 1-dimensional entity \\
Face & A 2-dimensional entity \\
Region & A 3-dimensional entity \\
Element & An entity not bounding another entity \\
(Hardware) Node & A computer having a single \\
 & hardware memory space, cooperating \\
 & with others via a network \\
Core & A CPU core or a GPU hardware thread \\
 & (the unit of hardware parallelism) \\
Process & An operating system process \\
 & (has a software memory space, \\
 & contains one or more threads) \\
Thread & An operating system thread \\
 & or a CUDA thread \\
 & (the unit of software parallelism) \\
(MPI) rank & One of several operating system \\
 & processes cooperating using MPI \\
 & to execute a parallel program \\
(mesh) partition & The subset of a mesh stored \\
 & in a single data structure. \\
(mesh) part & Synonym for mesh partition \\
\end{tabular}

\section{Notation}

\begin{tabular}{l|l}
$\Omega$ & a subset of Cartesian space (a domain) \\
$T^d$ & the subset of $d$-dimensional entities in a topological complex $T$ \\
$T^d_i$ & the $i$-th entity in $T^d$ \\
$\partial e$ & the set of lower-dimensional entities adjacent \\
 & to entity $e$, called its boundary \\
$\partial S$ & for a set of entities $S$, $\partial S = \bigcup_{e\in S} \partial e$ \\
$\bar{S}$ & for a set of entities $S$, $\bar{S} = S \cup \partial S$ \\
$a \subseteq \partial b$ & entity $a$ is a subset of entity $b$'s boundary, \\
 & meaning $a$ is adjacent to $b$ \\
$a\{T^q\}$ & the set of entities in $T^q$ adjacent to entity $a$ \\
$S\{T^q\}$ & for a set of entities $S$, $S\{T^q\} = \bigcup_{e\in S} e\{T^q\}$ \\
$\mathcal{M}$ & the (symmetric positive-definite) mesh metric tensor \\
$h$ & the desired length of mesh edges (isotropic size) \\
$\text{diag}(a,b,c)$ & a diagonal matrix $A\in\mathbb{R}^{3\times 3}$ with
   $A_{11}=a,A_{22}=b,A_{33}=c$ \\
$\lceil x\rceil$ & the smallest integer $\geq x$ \\
$l_e$ & the length of edge $e$ in real space \\
$\tilde{l}_e$ & the length of edge $e$ in metric space \\
$V_K$ & the volume of tetrahedron $K$ in real space \\
$\tilde{V}_K$ & the volume of tetrahedron $K$ in metric space \\
$l_{K,\text{RMS}}$ & the root-mean-squared edge length of tetrahedron $K$ \\
$\eta_K$ & the mean ratio quality of simplex $K$ \\ \hline
$P$ & the number of processes in an MPI job \\
$T$ & the number of threads per process \\ \hline
$\rho$ & density \\
$v$ & velocity \\
$\mathbf{p}$ & momentum \\
\end{tabular}

\section{Mesh Definitions}

We define here the basic concept of a \emph{mesh}
(Section \ref{sec:def_mesh}), based on the general
idea of a \emph{topological complex} (Section \ref{sec:def_complex})
and focusing on its \emph{adjacency relations} (Section \ref{sec:def_adj}).
Next we go on to specify the type of meshes we
are dealing with (Section \ref{sec:def_fem}),
and the unique way in which they are being modified,
i.e. mesh \emph{adaptation} (Section \ref{sec:def_adapt}).

\subsection{Topological Complex}
\label{sec:def_complex}

A point set is a subset of the points in some Cartesian
space $\mathbb{R}^D$.

A \emph{topological complex} $T$ is a set of point sets
containing points in $\mathbb{R}^D$.
Each point set $T^d_i$ in $T$ is an open subset of some
$d$-dimensional manifold embedded in $\mathbb{R}^D$,
where $0\leq d \leq D$.
We say that $d$ is the dimension of point set $T^d_i$.
We can denote all point sets of dimension $d$ in the
complex by $T^d$.

All point sets in $T$ are disjoint from one another,
and their union $\Omega = \bigcup T$ is a subset of some $D$-dimensional
manifold, i.e. a $D$-dimensional manifold with boundary.
We denote the boundary of this complex as $\Gamma = \partial\Omega$.
Each point set is an open subset of a $d$-manifold,
and the closed equivalent on said manifold,
denoted $\bar{T}^d_i = T^d_i \cup \partial T^d_i$,
is the open set plus its boundary.
Since the sets are disjoint, only their boundaries may intersect.
For all pairs of equal-dimension point sets, the intersection
of their boundaries must exist as the union of other,
lower-dimensional point sets in $T$:

\[\forall T^d_i,T^d_{j\neq i} \in T: \exists S \subseteq \{T^q_k \in T \big| q < d\}:
\bigcup S = \partial T^d_i \cap \partial T^d_j\]

Finally, to keep the surface properly divided, we require that
the intersection of any point set boundary with the overall
boundary also exist as a union of lower-dimensional point sets:

\[\forall T^d_i \in T: \exists S \subseteq \{T^q_k \in T \big| q < d\}:
\bigcup S = \partial T^d_i \cap \Gamma\]

Boundary-representation (BRep) CAD models are examples
of topological complexes, as are meshes.
Point sets of dimension 0 are called vertices, those
of dimension 1 are called edges, faces have dimension 2
and regions have dimension 3.
When discussing a topological complex, we
refer to point sets as \emph{entities}.

Figure \ref{fig:def_topo} illustrates two topological
complexes, both in 2D.
On the left is a cross-section of an airfoil in fluid,
showing how the airfoil forms a ``hole" in the fluid,
and a boundary may be represented by a single edge
that is curved and connects to only one unique vertex.
On the right is a simpler model of a triangular domain,
which is representative of how triangles are viewed
in a mesh.

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{def_topo.png}
\caption{Topological complexes: (left) an airfoil CAD model (right)
a single triangle}
\label{fig:def_topo}
\end{center}
\end{figure}

\subsection{Adjacency Relation}
\label{sec:def_adj}

Given a topological complex $T$, we can describe the relations between
point sets in terms of adjacency.
If a point set $b$ bounds a point set $a$,
$b \subseteq \partial a$, then we say there is a
downward adjacency $(a,b)$.
Note that downward adjacency is a transitive relation:

\[c \subseteq \partial b, b \subseteq \partial a \to c \subseteq \partial a\]

For every downward adjacency $(a,b)$, there exists an upward
adjacency $(b,a)$.
Together, upward and downward adjacencies defined this way
are called first-order adjacencies.

The first-order adjacency relations in a mesh define a graph,
which we call a topology graph.
The majority of our work is concerned with finding
efficient computer representations for topology graphs.

The topology subgraph between a pair of dimensions $T^p$, $T^q$
is a bipartite graph.
We have a notation for queries of this bipartite graph:
$T^p_i\{T^q\}$ is the set of entities (point sets) in $T^q$ adjacent to
$T^p_i$.
In general, one can query all entities adjacent
to a set of entities: $S\{T^q\} = \bigcup a \{T^q\}, a \in S$.
This makes it easier to define second-order adjacencies,
which are found by two transitive queries,
for example $T^a_i\{T^b\}\{T^c\}$.

Although these graphs have a natural direction for each
edge (from higher dimension to lower), we are interested
in being able to query both outgoing (downward) and
incoming (upward) relations, so the storage will be
bi-directional in many cases.

Another useful concept will be the {\it entity use},
which is essentially an edge of the topology graph.
If entity $b$ is in the boundary of entity $a$, then
$b$ is used by $a$, and that occurrence is an entity use.
The term shows up when data is stored once for every adjacency relation.

\subsection{Mesh}
\label{sec:def_mesh}

A {\it mesh} $M$ is herein defined as a special case of a topological
complex where the closure of each entity $\bar{M}^d_i$
is topologically a polytope of dimesion $d$.
Mesh entities which do not bound other entities
are called \emph{elements}.

Being polytopes topologically, mesh entities have no
holes or internal empty spaces,
so they do not need multiple loop or shell constructs to
describe their boundary the way a BRep CAD model would.

\subsection{Finite Element Mesh}
\label{sec:def_fem}

We further define a {\it finite element mesh} as a special case
of a mesh, with certain restrictions and requirements.
For our current purposes, a finite element mesh is composed
of entities whose closures are one of the following
polytope types:

\begin{enumerate}
\item point $(d = 0)$
\item line $(d = 1)$
\item triangle $(d = 2)$
\item quadrilateral $(d = 2)$
\item tetrahedron $(d = 3)$
\item hexahedron $(d = 3)$
\item (square-based) pyramid $(d = 3)$
\item triangular prism $(d = 3)$
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{zoo.png}
\caption{Polytopes commonly used in FE and FV meshes}
\label{fig:zoo}
\end{center}
\end{figure}

This list of polytopes is also illustrated in Figure
\ref{fig:zoo}, and
can be easily extended to include additional polytope
types of interest.

This work is focused on \emph{unstructured meshes}, meaning
their topology must be explicitly stored because it
is not originally defined by some simple pattern such as a grid.
Such unstructured meshes have an
advantage in representing complex geometry and in their
ability to easily vary resolution throughout the geometry.

In addition, the Finite Element Method uses fields which are each defined
as the weighted sum of finite number of basis functions,
where the weights are referred to as \emph{degrees of freedom} and are each
attached to one mesh entity.
This requires a data structure that can attach
degrees of freedom to mesh entities.

Finite element analysis procedures also require meshes where
the number of elements around some boundary entity (such as a vertex
or an edge) is limited to a reasonable upper bound,
otherwise shape quality and numerical conditioning will degrade.
Therefore, in such meshes, all upward adjacencies are bound
by a constant.
Any operation whose runtime is proportional to the
number of upward adjacencies can be treated as a constant-time operation
(see Appendix \ref{app:vert_up_deg} for a proof of this bound).

Finally, if there are multiple polytope types per dimension,
such as having both triangles and quadrilaterals in 2D, then
we say the mesh is {\it mixed}.

\subsubsection{Topological Template}
\label{sec:topo_template}

When dealing with stored mesh topology and attempting to describe
the relationships between adjacent entities, one needs some frame
of reference to begin with.
This takes the form of a \emph{topological template}
\cite{seol2005fmdb, karamete2016novel} which describes,
for a single polytope, a canonical numbering of its boundary entities.
For example, the vertices of a tetrahedron are numbered such that
the triple product $(x_1-x_0)\times(x_2-x_0)\cdot(x_3-x_0)$ yields
a positive value, where $x_i$ is the coordinate of vertex $i$.
Furthermore, the triangles bounding a tetrahedron have a canonical
ordering and orientation, for example the canonical second face of tetrahedron
$(a,b,c,d)$ can be defined as the triangle $(a,b,d)$.
Although these decisions are arbitrary, it is necessary to choose
such a template for each topological type in order to have
a frame of reference when programming operations that act on the boundary
of an entity.
It can also be useful to choose orderings to have certain special
properties that ease the programming of certain algorithms, for
example orienting all faces of an element such that the normal of
the face points outwards from the element.
See Section \ref{sec:osh_align} for an example of how this
information is used.

\subsection{Adaptation}
\label{sec:def_adapt}

There are two approaches to modifying the topology throughout
a simulation.
If an entirely new mesh is constructed, we say that the
method is {\it remeshing}.
If local changes are applied to the original mesh to transform
it into the new mesh, we say the method {\it adapts}.
Such local changes require adding and removing entities
from the mesh within local portions of the domain.
On the other hand, if the mesh is not changed during the
simulation, we say that the mesh is {\it static}.

Adaptation refers to a process of modifying the mesh by applying
mesh entity-level operations on mesh cavities.
We can define a {\it mesh cavity} as the
union of several mesh entities, in which the
mesh modification changes the interior (open set) of
the mesh entities within the cavity, leaving its boundary unchanged.

Adaptation has a number of benefits compared to complete remeshing.
Remeshing has a runtime cost at least proportional to the number
of total elements, while the cost of adaptation is only proportional
to the number of mesh entities modified.
Moreover, the transfer of field values from the old mesh
to the new mesh is a complex procedure when remeshing,
requiring spatial search algorithms and
tends to apply remapping operators that are diffusive and/or
have to deal with conservation requirements at a global level.

Adaptation by local mesh modification supports local execution
of solution transfer: refinement splits
parent entities and is able to transfer solution exactly using
shape function interpolation, and other operations are confined
to a local cavity so that any searching is fast,
and diffusive effects and conservation adjustments are local.

Local mesh adaptation requires unique properties of the
mesh data structure that are otherwise unnecessary for
static meshes.
If adaptation is programmed as a series of entity
additions and removals, then we require that \emph{entity
addition and removal be constant-time operations}.
Section \ref{sec:cavity_sched} presents two alternatives
to scheduling cavity operations, the latter of which
allows changes to be grouped into batches, which allows
the use of simpler data structures.

For several reasons, it is preferable to modify a cavity
by first constructing all new entities that fill the cavity,
which overlap with the old, and then destroying all old entities.
First, this allows both versions to be considered by a solution
transfer algorithm, which needs the mesh topology from both
to operate properly in the general case.
Second, we are able to evaluate quality and correctness metrics
of the new entities and, if those are unacceptable, cancel the
operation by destroying the new entities and leaving
the old entities in place.

As a consequence, the mesh structure must \emph{tolerate temporary
topological inconsistencies} introduced by adaptation.
For example, the first modification made is either the addition
of an entity which overlaps with existing entities
or the removal of an entity.
Adding an overlapping entity causes inconsistencies such as
a face which has three adjacent regions in the temporary mesh.
Removing an arbitrary entity can cause non-manifold configurations,
such as that of a vertex adjacent to only two elements, which
in turn only intersect at that vertex.

Certain modifications, such as edge collapsing, can only correctly
preserve the boundary of the mesh the mesh structure \emph{maintains
a direct mapping from mesh entities to geometric (CAD) model
entities}.
This mapping is referred to as classification \cite{schroeder1990combined}.
For example, sharp edges
can be preserved when it is known that a mesh entity is
on such a CAD model edge.
The inverse map of classification is called reverse
classification, and it defines the groups of mesh entities
to which boundary conditions are applied.

The implementations of edge collapsing which preserve
topological similarity require knowledge
of the classification for all mesh entities,
hence requiring a complete (though not necessarily full)
topological representation \cite{seol2006efficient}
to safely coarsen a mesh.
It would be possible to avoid storing some entities, so long
as the classification of entities not stored could be inferred.
Beyond that, it is convenient in any case to represent
edges explicitly, given that many adaptive algorithms
are based on edge lengths \cite{biswas1998tetrahedral}.
The adaptive applications in this work all use representations
that store all entities.

\subsection{Metric Field}
\label{sec:def_metric}

The most common way in which one describes the desired effects on
the mesh to an adaptation program is via a metric field
\cite{loseille2015parallel,compere2010mesh,li20053d}.
From a continuous perspective, the metric field is a tensor
$\mathcal{M}$ which varies over physical space.
This tensor describes a metric because at any given point
$\mathbf{x}$ in space, one can choose a direction expressed
as a unit vector $\mathbf{u}$, then the product
$\mathbf{u}^T\mathcal{M}(\mathbf{x})\mathbf{u}$ replaces the inner
product $\mathbf{u}^T\mathbf{u}$ for measuring distance along
the given direction.
We require the metric tensor to be a real symmetric positive-definite
$d\times d$ matrix for spatial dimension $d$, which means it
can be diagonalized by an orthogonal matrix and has all
positive eigenvalues, as expressed in Equation \ref{eq:metric_decomp}.

\begin{equation} \label{eq:metric_decomp}
\mathcal{M} = R^T\Lambda R,\, R^T R = I,\, \Lambda = \text{diag}(\lambda_1,\dots,\lambda_d),\,
\forall i\in [1,d]: \lambda_i > 0
\end{equation}

The matrix $R$ can be viewed as describing a rotation,
while the diagonal matrix $\Lambda$ scales a vector by a positive
amount along each coordinate axis.
One can then consider the matrix $Q$ as
defined by Equation \ref{eq:metric_transform} to be defining
an affine transformation (rotation followed by scaling) that
is applied to vectors before their inner product is taken,
as illustrated in Equation \ref{eq:metric_space_product}.

\begin{equation} \label{eq:metric_transform}
Q = \Lambda^{\frac12} R,\,
\Lambda^{\frac12} = \text{diag}(\sqrt{\lambda_1},\dots,\sqrt{\lambda_d})
\end{equation}

\begin{equation} \label{eq:metric_space_product}
\mathbf{u}^T \mathcal{M}\mathbf{v} =
\mathbf{u}^T R^T\Lambda R\mathbf{v} =
\mathbf{u}^T R^T\Lambda^{\frac12} \Lambda^{\frac12}R\mathbf{v} =
\mathbf{u}^T Q^T Q\mathbf{v} =
(Q\mathbf{u})^T (Q\mathbf{v}) =
\tilde{\mathbf{u}}^T \tilde{\mathbf{v}}
\end{equation}

Viewed in this way, the metric tensor is actually defining,
at each point in space, an affine transformation that should
be applied to vectors before computing local spatial qualities.
For us, it is sufficient to consider how the metric tensor
alters length along a direction $\mathbf{u}$ (Equation \ref{eq:metric_length}),
as well as volume (Equation \ref{eq:metric_volume}).
We use a tilde to denote quantities ``in metric space" (post-transformation).

\begin{equation} \label{eq:metric_length}
\tilde{l} = l \cdot \sqrt{\mathbf{u}^T\mathcal{M}\mathbf{u}}
\end{equation}

\begin{equation} \label{eq:metric_volume}
\tilde{V} = V \cdot \sqrt{\det(\mathcal{M})}
\end{equation}

Note that for many applications, a metric which is isotropic
may be sufficient, meaning that the entire tensor may be represented
by a single scalar $\lambda$, or for convenience a single
desired length value $h=1/\sqrt{\lambda}$.
It is useful for an adaptation code to consider this special case
separately because storage and algorithmic complexity are greatly
reduced (see Section \ref{sec:sf}).

\subsection{Element Quality}
\label{sec:def_quality}

Many researchers use a quality measure known as the mean ratio
to evaluate tetrahedra and triangles
\cite{liu1994relationship,loseille2015parallel,compere2010mesh,
li20053d}.
In Equation \ref{eq:tet_mean_ratio}, we define the mean ratio
in an equivalent but slightly different form than presented by other
authors.
This is to clearly show that the mean ratio may be interpreted
as comparing a tetrahedron's volume and the volume of
an equilateral tetrahedron with the same root-mean-squared edge length.

\begin{equation} \label{eq:tet_mean_ratio}
\eta_K = \left(\frac{V_K}{\gamma_K\cdot l_{K,\text{RMS}}^3}\right)^{\frac23},\,
l_{K,\text{RMS}}=\left(\frac16\sum_{i=1}^6 \left(l_{K,i}\right)^2\right)^\frac12, \,
\gamma_K = \frac{1}{\sqrt{72}}
\end{equation}

In Equation \ref{eq:tet_mean_ratio},
$V_K$ is the tetrahedron volume, $l_{K,i}$ is the
length of edge $i$ of the tetrahedron, and $\gamma_K$ is the volume
of an equilateral tetrahedron with unit edge length.
A similar definition exists for a triangle $T$
with area $A_T$ as shown in Equation \ref{eq:tri_mean_ratio}.

\begin{equation} \label{eq:tri_mean_ratio}
\eta_T = \frac{A_T}{\gamma_T\cdot l_{T,\text{RMS}}^2},\,
l_{T,\text{RMS}}=\left(\frac13\sum_{i=1}^3 \left(l_{T,i}\right)^2\right)^\frac12, \,
\gamma_T = \frac{\sqrt{3}}{4}
\end{equation}

\section{Reference Computer}
\label{sec:ref_comp}

Since the following sections will describe a variety
of different computer hardware systems, it is useful to have a reference
point of hardware to which they can be compared.
We define a ``reference computer" as follows:

\begin{enumerate}
\item It has a single CPU, which contains a set
of registers (places to store single values),
and can perform operations with the inputs
and outputs being registers.
Accessing a register is instant, but there are
only a dozen or so of them.
\item It has memory which is used to temporarily store
all the data required to solve the problem.
The CPU can transfer data between registers
and memory at a moderate cost.
\item It has a disk which is used to permanently store
inputs and outputs of the overall program.
The CPU can transfer data between memory
and the disk, at a high cost.
\item It has a user interface, in our case
we will simply say a human operator has
some way to receive data from and provide
data to the CPU in real time.
\item It has a network interface which transits
data to or receives data from other computers.
\item Only one of the above actions may be
performed at one time.
\item With the exception of a human or network choosing
to provide data, all choice of actions is
dictated by the CPU, which is in turn obeying
the instructions of a single stored program.
\end{enumerate}

We will also not pay much in-depth attention to the
architecture of the cache system, because it
is largely transparent to the program.
In particular, the program cannot explicitly
command the cache to behave in a certain way;
it can only indirectly influence the performance
of the cache in they way it transfers data to and
from memory.
The key performance principle regardless of cache design is
to make sure the order in which values are stored
in memory reflects the order in which they are accessed.

\section{Heterogeneous Node Architecture}
\label{sec:node_hw}

As mentioned in Section \ref{sec:intro}, a supercomputer is
a networked collection of nodes, and the design of a single node
is where much of their diversity has appeared in recent years.
This Section describes the three key hardware component developments
influencing this diversity.
Any present-day supercomputer node is a combination components
from each of these three categories.

\subsection{Multi-core CPUs}

Personal computers and similar mobile devices are limited
to a single CPU due to the evolution of their hardware and
software, so the computing power of a single CPU defines
the quality of the products from which vendors make
the majority of profits.
Given the technology at any point in time, a single CPU
core can only run so fast, and the way to add even more performance
has been to include multiple cores per CPU,
creating a miniature parallel system on a single chip
\cite{hennessy2011computer}.
At the time of this writing, most consumer devices
have two or four cores, and the high-end CPUs from
which the majority of clusters and previous generation
supercomputers are constructed
contain about sixteen cores.

A CPU core has its own set of registers, and more importantly
it has the full capability to dictate transfers to memory,
disks, the network, and the user interface.
In comparison with the reference computer from Section \ref{sec:ref_comp},
the key difference is that each core executes independently,
and therefore two cores can execute \emph{different} actions
at the \emph{same} time.

\subsection{GPU Coprocessors}
\label{sec:def_gpu}

For the past two decades, many personal computers came
equipped with pieces of specialized hardware called
Graphics Processing Units (GPUs).
These components were chips specifically hardwired to
perform the highly parallelizable operations involved
in displaying graphics, where each screen pixel could be handled
by a separate parallel processor.
As research in graphics progressed, there was a need to
move from hardwired to programmable graphics processing
to implement more realistic graphics algorithms.
GPUs whose operations were somewhat programmable began appearing
in 2002 such as NVidia's NV30 device.
By 2004, Stanford University had developed a useful programming
environment called BrookGPU \cite{buck2004brook}, which
enabled researchers to more easily use GPUs to solve new problems
and was subsequently adopted by AMD.
Usage spread from the graphics community to the closely
related scientific computing community.
GPUs were an affordable personal solution for researchers
to access substantial computing power.
The interest soon motivated NVidia to release a programming
environment called the Compute Unified Device Architecture (CUDA),
alongside its GeForce 8800 products in 2006.
By 2009, not only was CUDA was being seriously considered as a programming
environment to solve a wide variety of problems \cite{hwu2009compute},
but GPUs had made their way onto the world's top supercomputers.
China's Tianhe-1A computer, revealed in 2010,
contained over 7K NVidia GPUs, making it the world's
most powerful supercomputer until 2011 \cite{yang2011tianhe}.
American supercomputers soon followed suit, and in 2012
the Titan supercomputer at Oak Ridge National Laboratory
was equipped with over 14K NVidia GPUs \cite{bland2012titan},
making it the world's most powerful in the year 2012.
By this point, GPUs had thousands of ``cores" each.

In comparison to multi-core CPUs and the reference computer
in Section \ref{sec:ref_comp},
a GPU is in many ways limited in comparison to a multi-core CPU.
The GPU and its cores cannot directly access CPU memory,
disks, networks, or user interfaces.
Rather, GPUs have their own memory hardware, separate from that of the CPU,
and roughly equal in size.
GPU cores can only execute a small program called a \emph{kernel},
which is given to the GPU by the CPU.
Since the GPU accepts a different instruction set than the CPU,
kernel code must be compiled specifically for the GPU.
Other actions of the GPU are dictated at a high level
by the CPU, including transfer of data from CPU to GPU memory.
Due to the lack of other access, CPU-GPU data transfer is required
for network communication or disk file access.
To make matters worse, such data transfer is slow (low bandwidth)
compared to the rate at which the GPU can process data,
meaning that without careful implementation, one can spend
the majority of computer time simply waiting for data transfers
\cite{gregg2011data}.

Despite these limitations, the sheer number of cores in a GPU
combined with the efficiencies gained in exchange for lack of generality
means that GPUs can theoretically outperform CPUs by an order of
magnitude, and should be substantially more energy efficient.
This motivates researchers to re-design algorithms such that
their practical performance can approach this theoretical promise.

\subsection{The Intel Xeon Phi}

During the years that GPUs were being introduced into leading supercomputers,
Intel began developing a family of chips which were based on
their multi-core CPUs but tended more and more towards GPU-like capabilities,
by increasing the number of cores from tens to hundreds and while decreasing
the amount of power per core.
This family is called the Intel Xeon Phi line, and began with co-processors
that behaved like GPUs, requiring a host CPU to send them specific compute
requests.
In 2013, China presented their TianHe-2 supercomputer, whose nodes had two Intel
CPUs and three Intel Xeon Phi co-processors each.
With 16K total nodes, TianHe-2 remained the world's most powerful supercomputer
until 2016, the year of this writing.
Intel has since evolved their Xeon Phi co-processor into a full-fledged
processor with nearly one hundred CPU cores \cite{jeffers2013intel},
which will be used to construct near-future supercomputers in the United States.

A Xeon Phi processor can for the most part be considered a CPU with
many more cores than usual, with the additional benefit that they have
performance properties close to those of a single GPU.
Conversely, it may provide the hardware characteristic benefits of a GPU
with the generality of a CPU, since all cores can access main memory,
disks, and networks as described in Section \ref{sec:ref_comp}.

\section{Programming Environments}

When writing programs for heterogeneous supercomputers,
there are a few key programming environments which motivate
choices about the implementation.

\subsection{Operating System}

Even when given hardware as simple the reference
computer from Section \ref{sec:ref_comp}, one can emulate
the presence of more hardware by using an \emph{operating system},
which is a program that directly executes on the given computer
and in turn allows the execution of multiple other programs
simultaneously on that same computer.
The two operating system concepts most relevant to our work
are the \emph{process} and \emph{thread}.
An operating system process is an emulation of the reference computer:
it has its own memory space, which emulates hardware memory,
as well as access to disks, user interfaces, and networks,
which the operating system manages such that the process
appears to have unique access to these \emph{resources}.
A process also contains one or more threads, which are
software emulations of CPU cores.
They have their own emulated registers, and can execute
different actions at the same time.
A process has a single associated program whose instructions
the threads are following, although each thread may be
following a different subset of the instructions depending
on its own local state (registers and stack).
Operating system threads all have access to emulated process
resources such as networks in the same way that CPU cores
all have access to computer interfaces.

Despite having great complexity in their own right, operating
systems are fundamentally just sharing mechanisms.
Increasing the number of threads and processes that share
a fixed amount of hardware resources will degrade the emulated
performance of all the active threads as they will each be
provided with a smaller fraction of the hardware resources.

If one instead intends to devote all the hardware of a machine
to a single program, then it is natural to align the operating
system constructs with their physical counterparts.
This means binding an operating system process to a CPU, such
that its memory space maps to the CPU's entire available memory,
and binding operating system threads to CPU cores, such that
the amount of parallelism and the amount of memory sharing
perceived at the software level both
match the amount supported at the hardware level.

\subsection{MPI}
\label{sec:def_mpi}

Distributed memory computers have for the past two decades
been programmed using the Message Passing Interface (MPI)
\cite{hempel1994mpi,gropp1996mpi}.
This interface remains the most reliable and portable way to construct
programs for all distributed memory supercomputers,
and is thus central to any scalable program.
Although its early versions (version 2.0 and earlier) had
scalability challenges \cite{balaji2009mpi}, MPI has evolved
to effectively address these challenges and ``modern" MPI
as described in \cite{gropp2014using} scales well into the
million-core range.
We will present optimal usage of modern MPI features in
Section \ref{sec:paraops} and show in Section \ref{sec:phasta_active}
that one of the applications using tools developed
in this thesis scales to 768K cores using only MPI \cite{rasquinCise2014}.

MPI reflects in software the architecture of a distributed-memory
machine: a set of compute nodes which execute in parallel
and cooperate via a network.
Each node has its own memory must explicitly send network messages to provide
or query data stored in another node.
An MPI rank is an operating system process, and MPI provides a
consistent interface over the various operating system network interfaces.
Whereas operating system networking uses complex addresses and protocols
for generality, MPI maintains a concept of a group of $P$ ranks which are
cooperating and may address one another by a consecutive integer from
$0$ to $(P-1)$.
MPI provides many complex algorithms for coordinating such groups of ranks,
as well as a simplified method of transmitting data that allows programs
to focus on the contents of the data as opposed to the details of what
protocol may be used on the network.
In software terminology, MPI is implemented as a library, which means it
is simply a set of functions that can be called from a program.

\subsection{OpenMP}
\label{sec:openmp}

OpenMP is a programming environment mainly for multi-core CPUs.
In the same way that MPI is a software reflection of distributed memory,
OpenMP is a software reflection of shared memory.
It is based on the concept of a single operating system process
that contains $T$ operating system threads which are cooperating.
Since memory is shared within an operating system process, OpenMP
does not provide message-passing functionaliy the way MPI does.
What OpenMP does provide is the concept of a parallel {\bf for} loop,
which is a key abstraction for shared memory programming.
In structured serial programming, there is the concept of a {\bf for} loop.
Listing \ref{lst:for} illustrates a {\bf for} loop in the C++ language,
which adds a constant to the entries of an array {\tt a} and stores
the resulting values in the array {\tt b}.
This is done for a total of $n$ entries.
As written, a single thread does this work one entry after another.

\begin{lstlisting}[float,style=dan-style,caption=Serial {\bf for} loop,label=lst:for]
for (int i = 0; i < n; ++i) {
  b[i] = a[i] + 3.14159;
}
\end{lstlisting}

Unlike MPI, OpenMP is directive-based, meaning that it is integrated
directly into a programming language compiler in order to introduce
new language notation.
As Listing \ref{lst:omp_for} illustrates, this allows simple annotation
of the loop from Listing \ref{lst:for} which turns it into a parallel {\bf for} loop.
What now occurs is that all of the cooperating threads in OpenMP will share
the work of this loop.
Given $T$ cooperating threads, each thread will do the work for approximately
$(n/T)$ entries of the array, and OpenMP is responsible for the scheduling of
which entries each thread works on.
Assuming all threads are supported by proper CPU cores, the time to complete this
work should be $O(n/T)$, whereas the serial loop would have completed in $O(n)$ time.

\begin{lstlisting}[float,style=dan-style,caption=OpenMP parallel {\bf for} loop,label=lst:omp_for]
#pragma omp parallel for
for (int i = 0; i < n; ++i) {
  b[i] = a[i] + 3.14159;
}
\end{lstlisting}

Like any form of parallelism, this places restrictions on what serial
constructs may be parallelized.
For example, even a simple sum operation as shown in Listing \ref{lst:sum_for}
could not be parallelized in the same simple manner, because the {\tt sum} variable
is being read from and written to by multiple threads without the necessary coordination.
Such a situation is referred to as a \emph{race condition}, because depending on the
order in which different threads perform actions with respect to one another (which is indeterminate),
incorrect results may be produced.

\begin{lstlisting}[float,style=dan-style,caption=Dependent {\bf for} loop,label=lst:sum_for]
for (int i = 0; i < n; ++i) {
  sum = sum + a[i];
}
\end{lstlisting}

We refer to the execution of the body of the {\bf for} loop for a single
value of {\tt i} as an \emph{iteration}.
In order to avoid race conditions, we introduce the following principles of
array-based parallel {\bf for} loop programming:

\begin{enumerate}
\item Avoid writing to and reading from the same array in the same loop.
\item No array entry should be written to by more than one iteration.
\item If an array entry must be read from and written to, it must only
be accessed by a single iteration of the loop.
\item If the above rules must still be violated, it must be done via
atomic operations.
\end{enumerate}

Atomic operations exist to cover limited cases of multiple writes the same
entry by different threads or loop iterations.
For example, an atomic addition operation allows multiple iterations to
add values to the same array entry, with the guaranteed outcome that
after all iterations have executed, the correct sum appears at this entry.

\subsection{CUDA}
\label{sec:cuda}

CUDA is NVidia's programming environment for their large family of programmable GPUs \cite{nickolls2008scalable}.
Like OpenMP's parallel {\bf for} concept, CUDA's programming model is based on parallel execution
of kernels, which are almost the same as parallel {\bf for} loop iterations.
Listing \ref{lst:cuda_for} shows how one might transform Listing \ref{lst:for} into
the CUDA programming model.
Like OpenMP, CUDA is integrated into the compiler in order to introduce its own
non-standard language notation.
The loop iteration becomes its own function which must be properly annotated
to indicate it is a kernel.

\begin{lstlisting}[float,style=dan-style,caption=CUDA {\bf for} loop,label=lst:cuda_for]
__global__ add(double* a, double* b) {
  int i = blockIdx.x;
  b[i] = a[i] + 3.14159;
}
add<< n, 1 >>(a, b);
\end{lstlisting}

The limitations of GPUs described in Section \ref{sec:def_gpu} make their
way into the design of CUDA and software that uses it.
Annotations required on CUDA functions because the code in them
must be compiled specifically for the GPU.
CUDA also provides the functions with which the CPU controls GPU memory
allocations.
In Listing \ref{lst:cuda_for}, for example, both arrays {\tt a} and {\tt b}
need to be explicitly allocated in GPU memory.
As Section \ref{sec:def_gpu} mentions, CPU-GPU data transfer is slow and
should be avoided.
Also, for all practical purposes, kernel code should not allocate or
deallocate memory.
Although limited support for this exists, it should be avoided for best
performance.
This leads to the following principles of GPU programming:

\begin{enumerate}
\item Organize data in large allocations, such that the number of allocations
is not proportional to the size of the data. This allows the CPU to manage
allocations efficiently.
\item Allocate all data on the GPU, and only transfer it to the CPU
for network and disk operations.
\item A kernel should do a small, constant amount of work using a
small, constant amount of memory.
\end{enumerate}

The first and third principles have a serious impact on data structure
design, ruling out the many-small-objects approach.
Any data structures used locally at the kernel level should be designed such
that their size is known at compile time.
The second principle is essentially the most aggressive approach to avoid
CPU-GPU data transfer, and is motivated by the high cost of such data transfer.
Although it can be argued this is too restrictive,
we show in this thesis that very complex unstructured operations
can be carried out effectively while adhering to this principle.

\subsection{Kokkos}
\label{sec:kokkos}

Kokkos is a C++ library developed at Sandia National Labs that provides
a portable shared memory programming environment encompassing other
environments such as OpenMP and CUDA \cite{edwards2013kokkos}.
Kokkos maintains the parallel {\bf for} loop model of OpenMP while
accounting for the separation of code and data that may occur if CUDA is used.
Listing \ref{lst:kokkos_for} illustrates what the loop from Listing
\ref{lst:for} looks like when implemented using Kokkos.

\begin{lstlisting}[float,style=dan-style,caption=Kokkos {\bf for} loop,label=lst:kokkos_for]
auto add = KOKKOS_LAMBDA(int i) {
  b[i] = a[i] + 3.14159;
}
Kokkos::parallel_for(n, add);
\end{lstlisting}

The key here is that this code does not change when one moves
from multi-core CPUs using OpenMP to GPUs using CUDA.
Even for this simple example, the changes required would otherwise
be significant (compare Listings \ref{lst:omp_for} and \ref{lst:cuda_for}).

Kokkos also provides a data structure describing an array allocation
with features for controlling allocation to GPU memory if necessary
and explicit control over any transfers to the CPU.

\section{Overview of Software}

Below we present the various pieces of software which contain
the end products of the research and development done as
part of this thesis.

\subsection{PUMI}

The Parallel Unstructured Mesh Infrastructure (PUMI) is a software package
containing several libraries which together provide all the tooling
necessary to store, query, and adapt partitioned meshes with
simulation fields attached \cite{pumi_github}.
PUMI's design is based on locally serial software threads which
cooperate with one another using message passing, assuming
there is no sharing of memory.
Each software thread is required to have access to all computer
functions including memory allocation, message passing, file I/O, and the ability
to call any available third party library.
Because of these assumptions, PUMI's code could not execute
on more restricted architectures such as GPUs without very
substantial re-design and re-implementation.
Key components of PUMI include:

\subsubsection{PCU}

PCU is a C library which implements the key parallel communication
systems required to construct more complex parallel programs
which remain scalable.
Its algorithms and implementation will be covered in detail
in Section \ref{sec:pcu}.
PCU uses MPI to transmit all its messages, making it portable
across many distributed memory supercomputers.
PCU also made an effort to implement hybrid threading in
a way that was transparent to the rest of PUMI by providing
MPI-like communication between threads.

\subsubsection{APF}

APF is a C++ library whose original goal was to manage fields discretized
over a mesh.
In order to achieve that goal without being tied to a particular mesh
implementation, APF included an abstract interface for interacting
with any mesh implementation.
Because algorithms based on this interface are immediately able to
operate on multiple different mesh implementations, APF now contains
many such algorithms dealing with topological and parallel operations.

\subsubsection{MDS}

MDS is a C library implementing the main mesh data structure for APF and
therefore PUMI.
It is array-based, supports adding and removing entities in constant time,
can represent multiple element types at once, and is augmented with
parallel connectivity information for partitioned meshes.
This structure will be described in detail in Section \ref{sec:sisc}.

\subsubsection{MeshAdapt}

MeshAdapt is a C++ library that leverages PCU, APF, MDS, and other
libraries in PUMI to implement scalable parallel mesh adaptation.
MeshAdapt developments which are part of this thesis will be covered
in detail in Sections \ref{sec:ma_methods} as well as Section
\ref{sec:cavity_operator}.

\subsection{Omega\_h}

Omega\_h is a single C++ library which was designed from the beginning
with the restrictions and potential of GPU programming in mind
\cite{osh_github}.
It aims to provide as much of the core functionality of PUMI as
is feasible in a portably performant way, which currently
amounts to adapting triangle and tetrahedral meshes using anisotropic
metrics.
Omega\_h represents the first such portably performant code capable
of mesh adaptation, and is a major contribution of this thesis.
It uses the Kokkos library to achieve portable on-node parallelism
as described in Section \ref{sec:kokkos}, and techniques
it uses to produce deterministic results are presented
in Section \ref{sec:determinism}.
Its data structure will be described in Section \ref{sec:omega_h-struct}.
Its implementation of mesh adaption will be discussed mainly
in Section \ref{sec:omega_h-adapt}, with key parallel aspects
covered throughout Chapter \ref{chap:parallel}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
