%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                 %
%                            CHAPTER ONE                          %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{INTRODUCTION AND BACKGROUND}
\label{chap:intro}

\section{Introduction}
\label{sec:intro}

A wide variety of aerospace, mechanical, and nuclear engineering
problems require solving complex Partial Differential
Equations (PDEs) in time and space.
For efficiency and reliability, the solutions to these PDEs are
found using computers.

Computers are equipped with a limited amount of memory to
store information, and must use a mathematical representation
of a PDE solution that can be described using
a limited amount of information.
For many engineering problems of interest, the exact solution
as described by any known representation would require an
infinite amount of information, therefore approximate
solutions are sought.

A certain minimal amount of memory and processing power
are required to obtain the lowest accuracy approximate
solutions, and obtaining more accurate solutions requires
more memory and/or processing power.
For these reasons, computers with ever-increasing amounts
of memory and processing power are designed and built to
increase the accuracy of existing solutions and
to solve previously unsolvable engineering problems.
At any given point in history, the computers with the
most memory and processing power are called supercomputers.

Supercomputers are expensive to acquire and even more
expensive to operate in terms of electricity, cooling, and other
maintenance, so their ability to
produce accurate results at the lowest cost
is of critical importance.

When solving PDEs over general domains, it is common to
employ equation discretization methods that operate on a mesh,
which is more precisely defined in Section \ref{sec:def_mesh}.
Chapter \ref{chap:struct} presents the full design
and implementation of two new computer representations
of meshes, which are focused on minimizing the amount
of computer memory and processing required to use
an accurate mesh, and are also compatible with mesh adaptation.

Mesh adaptation is a way in which the spatial discretization
(mesh) can be altered over time to maximize the accuracy
of the solution obtainable with a certain amount
of computing power.
There are several open areas of research in mesh adaptation,
and Chapter \ref{chap:adapt} presents advancements
made in this work to design and implement
mesh adaptation that can run efficiently on present
and near-future supercomputers.

Present supercomputers make extensive use of distributed
memory parallelism by being constructed from tens of thousands of
smaller computers (nodes) connected by a fast network.
In an effort to reduce acquisition and maintenance costs
for a given amount of computing power,
present and future supercomputers will also make extensive use
of shared memory parallelism, by having each node be constructed
with hundreds to thousands of computing cores, all of which
are capable of working in parallel.
The combination of the two forms of parallelism is what
makes a supercomputer \emph{heterogeneous}.

Designing programs for heterogeneous supercomputers is a critical
challenge, because such programs must be able to precisely coordinate
a complex hierarchy of memory and interaction methods executing on
millions of compute cores
in order to solve a given problem at minimal cost.
Chapter \ref{chap:parallel} presents contributions to
the design and implementation of parallel programs,
including both widely applicable tools and tools specifically
designed to enable efficient parallel mesh adaptation.

An additional challenge to the design of parallel programs
is the fact that supercomputers fall into different
architectural design categories which currently
have very different programming interfaces.
Thus, in order to design a program which is \emph{portably performant}
over all architectures, one must try to abstract away
their differences and, unfortunately, cater to the
lowest common denominator of functionality.
Throughout this thesis, we present two systems:
the first only operates on some architectures and provides
a wide array of adaptive functionality, while the second
is portably performant across the major architectures
and currently provides less adaptive functionality.

Finally, Chapter \ref{chap:apps} presents several
application programs which make use of the tools developed
here in order to solve a variety of engineering problems.

\section{Nomenclature}

{\bf TO BE EXPANDED, once most content is in place}
{\bf Part of the nomenclature is attributed to SISC}

\begin{tabular}{l|l}
Topological Complex & A breakdown of a domain in Cartesian space into
topological entities \\
Mesh & A topological complex whose entities have simple shape \\
Entity & A topological entity of a mesh \\
Vertex & A 0-dimensional entity \\
Edge & A 1-dimensional entity \\
Face & A 2-dimensional entity \\
Region & A 3-dimensional entity \\
Element & An entity not bounding another entity \\
\end{tabular}

\section{Mesh Definitions}

We define here the basic concept of a \emph{mesh}
(Section \ref{sec:def_mesh}), based on the general
idea of a \emph{topological complex} (Section \ref{sec:def_complex})
and focusing on its \emph{adjacency relations} (Section \ref{sec:def_adj}).
Next we go on to specify the type of meshes we
are dealing with (Section \ref{sec:def_fem}),
and the unique way in which they are being modified,
i.e. mesh \emph{adaptation} (Section \ref{sec:def_adapt}).

{\bf All definitions before hardware are from SISC}

\subsection{Topological Complex}
\label{sec:def_complex}

A point set is a subset of the points in some Cartesian
space $\mathbb{R}^D$.

A \emph{topological complex} $T$ is a set of point sets
containing points in $\mathbb{R}^D$.
Each point set $T^d_i$ in $T$ is an open subset of some
$d$-dimensional manifold embedded in $\mathbb{R}^D$,
where $0\leq d \leq D$.
We say that $d$ is the dimension of point set $T^d_i$.
We can denote all point sets of dimension $d$ in the
complex by $T^d$.

All point sets in $T$ are disjoint from one another,
and their union $\Omega = \bigcup T$ is a subset of some $D$-dimensional
manifold, i.e. a $D$-dimensional manifold with boundary.
We denote the boundary of this complex as $\Gamma = \partial\Omega$.
Each point set is an open subset of a $d$-manifold,
and the closed equivalent on said manifold,
denoted $\bar{T}^d_i = T^d_i \cup \partial T^d_i$,
is the open set plus its boundary.
Since the sets are disjoint, only their boundaries may intersect.
For all pairs of equal-dimension point sets, the intersection
of their boundaries must exist as the union of other,
lower-dimensional point sets in $T$:

\[\forall T^d_i,T^d_{j\neq i} \in T: \exists S \subseteq \{T^q_k \in T \big| q < d\}:
\bigcup S = \partial T^d_i \cap \partial T^d_j\]

Finally, to keep the surface properly divided, we require that
the intersection of any point set boundary with the overall
boundary also exist as a union of lower-dimensional point sets:

\[\forall T^d_i \in T: \exists S \subseteq \{T^q_k \in T \big| q < d\}:
\bigcup S = \partial T^d_i \cap \Gamma\]

Boundary-representation (BRep) CAD models are examples
of topological complexes, as are meshes.
Point sets of dimension 0 are called vertices, those
of dimension 1 are called edges, faces have dimension 2
and regions have dimension 3.
When discussing a topological complex, we
refer to point sets as \emph{entities}.

Figure \ref{fig:def_topo} illustrates two topological
complexes, both in 2D.
On the left is a cross-section of an airfoil in fluid,
showing how the airfoil forms a ``hole" in the fluid,
and a boundary may be represented by a single edge
that is curved and connects to only one unique vertex.
On the right is a simpler model of a triangular domain,
which is representative of how triangles are viewed
in a mesh.

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{def_topo.png}
\caption{Topological complexes: (left) an airfoil CAD model (right)
a single triangle}
\label{fig:def_topo}
\end{center}
\end{figure}

\subsection{Adjacency Relation}
\label{sec:def_adj}

Given a topological complex $T$, we can describe the relations between
point sets in terms of adjacency.
If a point set $b$ bounds a point set $a$,
$b \subseteq \partial a$, then we say there is a
downward adjacency $(a,b)$.
Note that downward adjacency is a transitive relation:

\[c \subseteq \partial b, b \subseteq \partial a \to c \subseteq \partial a\]

For every downward adjacency $(a,b)$, there exists an upward
adjacency $(b,a)$.
Together, upward and downward adjacencies defined this way
are called first-order adjacencies.

The first-order adjacency relations in a mesh define a graph,
which we call a topology graph.
The majority of our work is concerned with finding
efficient computer representations for topology graphs.

The topology subgraph between a pair of dimensions $T^p$, $T^q$
is a bipartite graph.
We have a notation for queries of this bipartite graph:
$T^p_i\{T^q\}$ is the set of entities (point sets) in $T^q$ adjacent to
$T^p_i$.
In general, one can query all entities adjacent
to a set of entities: $S\{T^q\} = \bigcup a \{T^q\}, a \in S$.
This makes it easier to define second-order adjacencies,
which are found by two transitive queries,
for example $T^a_i\{T^b\}\{T^c\}$.

Although these graphs have a natural direction for each
edge (from higher dimension to lower), we are interested
in being able to query both outgoing (downward) and
incoming (upward) relations, so the storage will be
bi-directional in many cases.

Another useful concept will be the {\it entity use},
which is essentially an edge of the topology graph.
If entity $b$ is in the boundary of entity $a$, then
$b$ is used by $a$, and that occurrence is an entity use.
The term shows up when data is stored once for every adjacency relation.

\subsection{Mesh}
\label{sec:def_mesh}

A {\it mesh} $M$ is herein defined as a special case of a topological
complex where the closure of each entity $\bar{M}^d_i$
is topologically a polytope of dimesion $d$.
Mesh entities which do not bound other entities
are called \emph{elements}.

Being polytopes topologically, mesh entities have no
holes or internal empty spaces,
so they do not need multiple loop or shell constructs to
describe their boundary the way a BRep CAD model would.

\subsection{Finite Element Mesh}
\label{sec:def_fem}

We further define a {\it finite element mesh} as a special case
of a mesh, with certain restrictions and requirements.
For our current purposes, a finite element mesh is composed
of entities whose closures are one of the following
polytope types:

\begin{enumerate}
\item point $(d = 0)$
\item line $(d = 1)$
\item triangle $(d = 2)$
\item quadrilateral $(d = 2)$
\item tetrahedron $(d = 3)$
\item hexahedron $(d = 3)$
\item (square-based) pyramid $(d = 3)$
\item triangular prism $(d = 3)$
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{zoo.png}
\caption{Polytopes commonly used in FE and FV meshes}
\label{fig:zoo}
\end{center}
\end{figure}

This list of polytopes is also illustrated in Figure
\ref{fig:zoo}, and
can be easily extended to include additional polytope
types of interest.

This work is focused on \emph{unstructured meshes}, meaning
their topology must be explicitly stored because it
is not originally defined by some simple pattern such as a grid.
Such unstructured meshes have an
advantage in representing complex geometry and in their
ability to easily vary resolution throughout the geometry.

In addition, the Finite Element Method uses fields which are each defined
as the weighted sum of finite number of basis functions,
where the weights are referred to as \emph{degrees of freedom} and are each
attached to one mesh entity.
This requires a data structure that can attach
degrees of freedom to mesh entities.

Finite element analysis procedures also require meshes where
the number of elements around some boundary entity (such as a vertex
or an edge) is limited to a reasonable upper bound,
otherwise shape quality and numerical conditioning will degrade.
Therefore, in such meshes, all upward adjacencies are bound
by a constant.
Any operation whose runtime is proportional to the
number of upward adjacencies can be treated as a constant-time operation.

{\bf consider including a more detailed proof of constant upward
adjacency degree, which I already have written somewhere}

Finally, if there are multiple polytope types per dimension,
such as having both triangles and quadrilaterals in 2D, then
we say the mesh is {\it mixed}.

\subsection{Adaptation}
\label{sec:def_adapt}

There are two approaches to modifying the topology throughout
a simulation.
If an entirely new mesh is constructed, we say that the
method is {\it remeshing}.
If local changes are applied to the original mesh to transform
it into the new mesh, we say the method {\it adapts}.
Such local changes require adding and removing entities
from the mesh within local portions of the domain.
On the other hand, if the mesh is not changed during the
simulation, we say that the mesh is {\it static}.

Adaptation refers to a process of modifying the mesh by applying
mesh entity-level operations on mesh cavities.
We can define a {\it mesh cavity} as the
union of several mesh entities, in which the
mesh modification changes the interior (open set) of
the mesh entities within the cavity, leaving its boundary unchanged.

Adaptation has a number of benefits compared to complete remeshing.
Remeshing has a runtime cost at least proportional to the number
of total elements, while the cost of adaptation is only proportional
to the number of mesh entities modified.
Moreover, the transfer of field values from the old mesh
to the new mesh is a complex procedure when remeshing,
requiring spatial search algorithms and
tends to apply remapping operators that are diffusive and/or
have to deal with conservation requirements at a global level.

Adaptation by local mesh modification supports local execution
of solution transfer: refinement splits
parent entities and is able to transfer solution exactly using
shape function interpolation, and other operations are confined
to a local cavity so that any searching is fast,
and diffusive effects and conservation adjustments are local.

Local mesh adaptation requires unique properties of the
mesh data structure that are otherwise unnecessary for
static meshes.
If adaptation is programmed as a series of entity
additions and removals, then we require that \emph{entity
addition and removal be constant-time operations}.
Section \ref{sec:cavity_sched} presents two alternatives
to scheduling cavity operations, the latter of which
allows changes to be grouped into batches, which allows
the use of simpler data structures.

For several reasons, it is preferable to modify a cavity
by first constructing all new entities that fill the cavity,
which overlap with the old, and then destroying all old entities.
First, this allows both versions to be considered by a solution
transfer algorithm, which needs the mesh topology from both
to operate properly in the general case.
Second, we are able to evaluate quality and correctness metrics
of the new entities and, if those are unacceptable, cancel the
operation by destroying the new entities and leaving
the old entities in place.

As a consequence, the mesh structure must \emph{tolerate temporary
topological inconsistencies} introduced by adaptation.
For example, the first modification made is either the addition
of an entity which overlaps with existing entities
or the removal of an entity.
Adding an overlapping entity causes inconsistencies such as
a face which has three adjacent regions in the temporary mesh.
Removing an arbitrary entity can cause non-manifold configurations,
such as that of a vertex adjacent to only two elements, which
in turn only intersect at that vertex.

Certain modifications, such as edge collapsing, can only correctly
preserve the boundary of the mesh the mesh structure \emph{maintains
a direct mapping from mesh entities to geometric (CAD) model
entities}.
This mapping is referred to as classification \cite{schroeder1990combined}.
For example, sharp edges
can be preserved when it is known that a mesh entity is
on such a CAD model edge.
The inverse map of classification is called reverse
classification, and it defines the groups of mesh entities
to which boundary conditions are applied.

The implementations of edge collapsing which preserve
topological similarity require knowledge
of the classification for all mesh entities,
hence requiring a complete (though not necessarily full)
topological representation \cite{seol2006efficient}
to safely coarsen a mesh.
It would be possible to avoid storing some entities, so long
as the classification of entities not stored could be inferred.
Beyond that, it is convenient in any case to represent
edges explicitly, given that many adaptive algorithms
are based on edge lengths \cite{biswas1998tetrahedral}.
The adaptive applications in this work all use representations
that store all entities.

\section{Reference Computer}
\label{sec:ref_comp}

Since the following sections will describe a variety
of different computer hardware systems, it is useful to have a reference
point of hardware to which they can be compared.
We define a ``reference computer" as follows:

\begin{enumerate}
\item It has a single CPU, which contains a set
of registers (places to store single values),
and can perform operations with the inputs
and outputs being registers.
Accessing a register is instant, but there are
only a dozen or so of them.
\item It has memory which is used to temporarily store
all the data required to solve the problem.
The CPU can transfer data between registers
and memory at a moderate cost.
\item It has a disk which is used to permanently store
inputs and outputs of the overall program.
The CPU can transfer data between memory
and the disk, at a high cost.
\item It has a user interface, in our case
we will simply say a human operator has
some way to receive data from and provide
data to the CPU in real time.
\item It has a network interface which transits
data to or receives data from other computers.
\item Only one of the above actions may be
performed at one time.
\item With the exception of a human or network choosing
to provide data, all choice of actions is
dictated by the CPU, which is in turn obeying
the instructions of a single stored program.
\end{enumerate}

We will also not pay much in-depth attention to the
architecture of the cache system, because it
is largely transparent to the program.
In particular, the program cannot explicitly
command the cache to behave in a certain way;
it can only indirectly influence the performance
of the cache in they way it transfers data to and
from memory.
The key performance principle regardless of cache design is
to make sure the order in which values are stored
in memory reflects the order in which they are accessed.

\section{Heterogeneous Node Architecture}

As mentioned in Section \ref{sec:intro}, a supercomputer is
a networked collection of nodes, and the design of a single node
is where much of their diversity has appeared in recent years.
This Section describes the three key hardware component developments
influencing this diversity.
Any present-day supercomputer node is a combination components
from each of these three categories.

\subsection{Multi-core CPUs}

Personal computers and similar mobile devices are limited
to a single CPU due to the evolution of their hardware and
software, so the computing power of a single CPU defines
the quality of the products from which vendors make
the majority of profits.
Given the technology at any point in time, a single CPU
core can only run so fast, and the way to add even more performance
has been to include multiple cores per CPU,
creating a miniature parallel system on a single chip
\cite{hennessy2011computer}.
At the time of this writing, most consumer devices
have two or four cores, and the high-end CPUs from
which the majority of clusters and supercomputers are constructed
contain about sixteen cores.

A CPU core has its own set of registers, and more importantly
it has the full capability to dictate transfers to memory,
disks, the network, and the user interface.
In comparison with the reference computer from Section \ref{sec:ref_comp},
the key difference is that each core executes independently,
and therefore two cores can execute \emph{different} actions
at the \emph{same} time.

\subsection{GPU Coprocessors}

For the past two decades, many personal computers came
equipped with pieces of specialized hardware called
Graphics Processing Units (GPUs).
These components were chips specifically hardwired to
perform the highly parallelizable operations involved
in displaying graphics, where each screen pixel could be handled
by a separate parallel processor.
As research in graphics progressed, there was a need to
move from hardwired to programmable graphics processing
to implement more realistic graphics algorithms.
GPUs whose operations were somewhat programmable began appearing
in 2002 such as NVidia's NV30 device.
By 2004, Stanford University had developed a useful programming
environment called BrookGPU \cite{buck2004brook}, which
enabled researchers to more easily use GPUs to solve new problems
and was subsequently adopted by AMD.
Usage spread from the graphics community to the closely
related scientific community.
GPUs were an affordable personal solution for researchers
to access substantial computing power.
The interest soon motivated NVidia to release a programming
environment called the Compute Unified Device Architecture (CUDA),
alongside its GeForce 8800 products in 2006.
By 2009, not only was CUDA was being seriously considered as a programming
environment to solve a wide variety of problems \cite{hwu2009compute},
but GPUs had made their way onto the world's top supercomputers.
China's Tianhe-1A computer, revealed in 2010,
contained over 7K NVidia GPUs, making it the world's
most powerful supercomputer until 2011 \cite{yang2011tianhe}.
American supercomputers soon followed suit, and in 2012
the Titan supercomputer at Oak Ridge National Laboratory
was equipped with over 14K NVidia GPUs \cite{bland2012titan},
making it the world's most powerful in the year 2012.
By this point, GPUs had thousands of ``cores" each.

In comparison to multi-core CPUs and the reference computer
in Section \ref{sec:ref_comp},
a GPU is in many ways limited in comparison to a multi-core CPU.
The GPU and its cores cannot directly access main memory,
disks, networks, or user interfaces.
Rather, GPUs have their own memory hardware, separate from that of the CPU,
and this is the only memory or interface that GPU cores may access.
Furthermore, the actions of the GPU are for the most part dictated
by the CPU, including transfer of data from CPU to GPU memory.
GPU cores all simultaneously execute a small program called a \emph{kernel},
which is given to the GPU by the CPU.
Despite these limitations, the sheer number of cores in a GPU
combined with the efficiencies gained from lack of generality
make it significantly more powerful than any normal CPU.

\subsection{The Intel Xeon Phi}

During the years that GPUs were being introduced into leading supercomputers,
Intel began developing a family of chips which were based on
their multi-core CPUs but tended more and more towards GPU-like capabilities,
by increasing the number of cores from tens to hundreds and while decreasing
the amount of power per core.
This family is called the Intel Xeon Phi line, and began with co-processors
that behaved like GPUs, requiring a host CPU to send them specific compute
requests.
In 2013, China presented their TianHe-2 supercomputer, whose nodes had two Intel
CPUs and three Intel Xeon Phi co-processors each.
With 16K total nodes, TianHe-2 remained the world's most powerful supercomputer
until 2016, the year of this writing.
Intel has since evolved their Xeon Phi co-processor into a full-fledged
processor with nearly one hundred CPU cores \cite{jeffers2013intel},
which will be used to construct near-future supercomputers in the United States.

A Xeon Phi processor can for the most part be considered a CPU with
many more cores than usual, with the additional benefit that they have
performance properties close to those of a single GPU.
Conversely, it may provide the hardware characteristic benefits of a GPU
with the generality of a CPU, since all cores can access main memory,
disks, and networks as described in Section \ref{sec:ref_comp}.

\section{Programming Environments}

When writing programs for heterogeneous supercomputers,
there are a few key programming environments which motivate
choices about the implementation.

\subsection{Operating System}

Even when given hardware as simple the reference
computer from Section \ref{sec:ref_comp}, one can emulate
the presence of more hardware by using an \emph{operating system},
which is a program that directly executes on the given computer
and in turn allows the execution of multiple other programs
simultaneously on that same computer.
This was one of the main reasons for the development of operating
systems; to enable multi-tasking on otherwise single-task hardware.
The two operating system concepts most relevant to our work
are the \emph{process} and \emph{thread}.
An operating system process is an emulation of the reference computer:
it has its own memory space, which emulates hardware memory,
as well as access to disks, user interfaces, and networks,
which the operating system manages such that the process
appears to have unique access to these \emph{resources}.
A process also contains one or more threads, which are
software emulations of CPU cores.
They have their own emulated registers, and can execute
different actions at the same time.
A process has a single associated program whose instructions
the threads are following, although each thread may be
following a different subset of the instructions depending
on its own local state (registers and stack).
Operating system threads all have access to all emulated process
resources such as networks in the same way that each CPU core
has access to all computer interfaces.

Despite having great complexity in their own right, operating
systems are fundamentally just sharing mechanisms, and
increasing the number of threads and processes that share
a fixed amount of hardware resources will degrade the emulated
performance of all the active threads.

\subsection{MPI}

Distributed memory computers have for the past two decades
been programmed using the Message Passing Interface (MPI)
\cite{hempel1994mpi,walker1996mpi}.
This interface remains the only reliable and portable way to construct
programs for all distributed memory supercomputers,
and is thus central to any scalable program.
Although its early versions (version 2.0 and earlier) had
scalability challenges \cite{balaji2009mpi}, MPI has evolved
to effectively address these challenges and ``modern" MPI
as described in \cite{gropp2014using} scales well into the
million-core range.
We will present optimal usage of modern MPI features in
Section \ref{sec:mpi3} and show in Section \ref{sec:phasta_active}
that one of the applications using tools developed
in this thesis scales to 768K cores using only MPI \cite{rasquinCise2014}.

MPI reflects in software the architecture of a distributed-memory
machine: a set of compute nodes which execute in parallel
and cooperate via a network.
Each node has its own memory must explicitly send network messages to provide
or query data stored in another node.
An MPI rank is an operating system process; a software concept
which encompasses a memory space and one or more threads of execution.

\subsection{OpenMP}

OpenMP is a programming environment mainly for multi-core CPUs.

\subsection{CUDA}

This reference describes the CUDA environment: \cite{nickolls2008scalable}.

\section{Goals}
\label{sec:intro_goals}

{\bf These goals are from the SISC paper in review, will
need to be attributed}

An unstructured mesh simulation code relies heavily on
multiple core capabilities to deal with the mesh,
and the range of features available at this level constrain
the capabilities of the simulation as a whole.
As such, the long-term goal towards which this thesis
contributes is the development of a mesh handling system
with the following capabilities:

\begin{enumerate}
\item The flexibility to adapt to evolving meshes
\item The ability to represent any of the conforming meshes typically
used by Finite Element (FE) and Finite Volume (FV) methods
\item Low memory use
\item High locality of storage
\item Highly scalable implementation for distributed memory computers
\item The ability to parallelize work inside heterogeneous
supercomputer nodes
\end{enumerate}

The first goal is the most consequential; supporting adaptivity
is the reason for much of the complexity in the structure
and its difference compared to many non-adaptive mesh structures
(see Section \ref{sec:def_adapt} for further discussion).

\section{Overview of Software}

Below we present the various pieces of software which contain
the end products of the research and development done as
part of this thesis.

\subsection{PUMI}

The Parallel Unstructured Mesh Infrastructure (PUMI) is a software package
containing several libraries which together provide all the tooling
necessary to store, query, and adapt partitioned meshes with
simulation fields attached.
PUMI's design is based on locally serial software threads which
cooperate with one another using message passing, assuming
there is no sharing of memory.
Each software thread is required to have access to all computer
functions including memory allocation, message passing, file I/O, and the ability
to call any available third party library.
Because of these assumptions, PUMI's code could not execute
on more restricted architectures such as GPUs without very
substantial re-design and re-implementation.
Key components of PUMI include:

\subsubsection{PCU}

PCU is a C library which implements the key parallel communication
systems required to construct more complex parallel programs
which remain scalable.
Its algorithms and implementation will be covered in detail
in Section \ref{sec:pcu}.
PCU uses MPI to transmit all its messages, making it portable
across many distributed memory supercomputers.
PCU also made an effort to implement hybrid threading in
a way that was transparent to the rest of PUMI by providing
MPI-like communication between threads.

\subsubsection{APF}

APF is a C++ library whose original goal was to manage fields discretized
over a mesh.
In order to achieve that goal without being tied to a particular mesh
implementation, APF included an abstract interface for interacting
with any mesh implementation.
Because algorithms based on this interface are immediately able to
operate on multiple different mesh implementations, APF now contains
many such algorithms dealing with topological and parallel operations.

\subsubsection{MDS}

MDS is a C library implementing the main mesh data structure for APF and
therefore PUMI.
It is array-based, supports adding and removing entities in constant time,
can represent multiple element types at once, and is augmented with
parallel connectivity information for partitioned meshes.
This structure will be described in detail in Section \ref{sec:sisc}.

\subsubsection{MeshAdapt}

MeshAdapt is a C++ library that leverages PCU, APF, MDS, and other
libraries in PUMI to implement scalable parallel mesh adaptation.
MeshAdapt developments which are part of this thesis will be covered
in detail in Sections \ref{sec:ma_methods} as well as Section
\ref{sec:cavity_operator}.

\subsection{Omega\_h}

Omega\_h is a single C++ library which was designed from the beginning
with the restrictions and potential of GPU programming in mind.
It aims to provide as much of the core functionality of PUMI as
is feasible in a portably performant way, which currently
amounts to adapting triangle and tetrahedral meshes to anisotropic
metrics.
Omega\_h represents the first such portably performant code capable
of mesh adaptation, and is a major contribution of this thesis.
Its fundamental approach to on-node parallelism is
presented in Section \ref{sec:parallel_for}, and techniques
used to maintain deterministic execution are presented
in Section \ref{sec:determinism}.
Its data structure will be described in Section \ref{sec:omega_h-struct}.
Its implementation of mesh adaption will be discussed mainly
in Section \ref{sec:omega_h-adapt}, with key parallel aspects
covered throughout Chapter \ref{chap:parallel}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
